{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.7. Deep Recommender Systems\n",
    "\n",
    "Deep learning has revolutionized recommender systems by enabling the\n",
    "modeling of complex, non-linear relationships in user-item interactions.\n",
    "This section explores the application of deep neural networks to\n",
    "recommendation problems, providing both theoretical foundations and\n",
    "practical implementations.\n",
    "\n",
    "## 13.7.1. Introduction to Deep Recommender Systems\n",
    "\n",
    "### Motivation and Problem Formulation\n",
    "\n",
    "Traditional collaborative filtering methods have fundamental limitations\n",
    "that deep learning addresses:\n",
    "\n",
    "#### Limitations of Traditional Methods\n",
    "\n",
    "1.  **Linear Assumptions**: Matrix factorization assumes linear\n",
    "    relationships between user and item latent factors\n",
    "    - **Mathematical Limitation**:\n",
    "      $\\hat{r}_{ui} = \\mathbf{u}_u^T \\mathbf{v}_i$ only captures linear\n",
    "      interactions\n",
    "    - **Real-world Reality**: User preferences often exhibit complex,\n",
    "      non-linear patterns\n",
    "2.  **Manual Feature Engineering**: Requires domain expertise to extract\n",
    "    meaningful features\n",
    "    - **Time-consuming**: Engineers must manually design features for\n",
    "      each domain\n",
    "    - **Domain-specific**: Features that work for movies may not work\n",
    "      for books\n",
    "3.  **Cold Start Problems**: Poor performance with sparse data or new\n",
    "    users/items\n",
    "    - **Mathematical Challenge**: Insufficient data points for reliable\n",
    "      parameter estimation\n",
    "    - **Practical Impact**: New users receive poor recommendations\n",
    "4.  **Limited Scalability**: Difficulty handling complex patterns and\n",
    "    multi-modal data\n",
    "    - **Computational Bottleneck**: Traditional methods struggle with\n",
    "      high-dimensional data\n",
    "    - **Feature Integration**: Limited ability to combine text, images,\n",
    "      and other data types\n",
    "\n",
    "#### Deep Learning Solutions\n",
    "\n",
    "Deep learning addresses these limitations through:\n",
    "\n",
    "1.  **Non-linear Modeling**: Neural networks can approximate any\n",
    "    continuous function\n",
    "    - **Universal Approximation**: For any continuous function\n",
    "      $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, there exists a neural\n",
    "      network that can approximate it arbitrarily well\n",
    "    - **Complex Interactions**: Can capture high-order interactions\n",
    "      between features\n",
    "2.  **Automatic Feature Learning**: Networks discover optimal\n",
    "    representations automatically\n",
    "    - **End-to-end Learning**: Features are learned jointly with the\n",
    "      prediction task\n",
    "    - **Hierarchical Representations**: Multiple layers capture features\n",
    "      at different abstraction levels\n",
    "3.  **Multi-modal Integration**: Can handle various data types\n",
    "    simultaneously\n",
    "    - **Unified Framework**: Text, images, audio, and structured data\n",
    "      can be processed together\n",
    "    - **Cross-modal Learning**: Relationships between different data\n",
    "      modalities can be learned\n",
    "4.  **Scalability**: Can handle large-scale data efficiently\n",
    "    - **Parallel Processing**: GPU acceleration enables training on\n",
    "      massive datasets\n",
    "    - **Distributed Training**: Can be trained across multiple machines\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "#### Function Approximation Theory\n",
    "\n",
    "Deep recommender systems learn a function that maps user-item-context\n",
    "tuples to predictions:\n",
    "\n",
    "$$f: \\mathcal{U} \\times \\mathcal{I} \\times \\mathcal{C} \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "where: - $\\mathcal{U}$ is the user space (user IDs, features,\n",
    "demographics) - $\\mathcal{I}$ is the item space (item IDs, features,\n",
    "categories) - $\\mathcal{C}$ is the context space (time, location,\n",
    "device, etc.) - $\\mathbb{R}$ is the prediction space (rating,\n",
    "probability, ranking score)\n",
    "\n",
    "#### Universal Approximation Theorem\n",
    "\n",
    "**Theorem**: Let $\\sigma$ be a continuous, bounded, and non-constant\n",
    "activation function. Then for any continuous function\n",
    "$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ and any $\\epsilon > 0$, there\n",
    "exists a neural network with one hidden layer that can approximate $f$\n",
    "with error less than $\\epsilon$.\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "$$\\left| f(\\mathbf{x}) - \\sum_{i=1}^N \\alpha_i \\sigma(\\mathbf{w}_i^T \\mathbf{x} + b_i) \\right| < \\epsilon$$\n",
    "\n",
    "This theorem justifies why neural networks can capture complex\n",
    "recommendation patterns.\n",
    "\n",
    "#### Representation Learning\n",
    "\n",
    "Deep networks learn hierarchical representations:\n",
    "\n",
    "1.  **Low-level Features**: Raw input processing (user IDs, item IDs)\n",
    "2.  **Mid-level Features**: Interaction patterns and preferences\n",
    "3.  **High-level Features**: Abstract user preferences and item\n",
    "    characteristics\n",
    "\n",
    "**Mathematical Representation**:\n",
    "$$\\mathbf{h}^{(l+1)} = \\sigma(\\mathbf{W}^{(l)} \\mathbf{h}^{(l)} + \\mathbf{b}^{(l)})$$\n",
    "\n",
    "where $\\mathbf{h}^{(l)}$ is the representation at layer $l$.\n",
    "\n",
    "#### Loss Function Design\n",
    "\n",
    "The choice of loss function depends on the recommendation task:\n",
    "\n",
    "1.  **Rating Prediction** (Regression):\n",
    "    $$\\mathcal{L}_{\\text{MSE}} = \\frac{1}{N} \\sum_{(u,i) \\in \\mathcal{R}} (r_{ui} - \\hat{r}_{ui})^2$$\n",
    "\n",
    "2.  **Click Prediction** (Binary Classification):\n",
    "    $$\\mathcal{L}_{\\text{BCE}} = -\\frac{1}{N} \\sum_{(u,i) \\in \\mathcal{R}} [r_{ui} \\log(\\hat{r}_{ui}) + (1-r_{ui}) \\log(1-\\hat{r}_{ui})]$$\n",
    "\n",
    "3.  **Ranking** (Pairwise Learning):\n",
    "    $$\\mathcal{L}_{\\text{BPR}} = -\\sum_{(u,i,j) \\in \\mathcal{D}} \\log(\\sigma(\\hat{r}_{ui} - \\hat{r}_{uj}))$$\n",
    "\n",
    "where $\\mathcal{D}$ contains triples $(u,i,j)$ where user $u$ prefers\n",
    "item $i$ over item $j$.\n",
    "\n",
    "### Architectural Principles\n",
    "\n",
    "#### 1. Embedding Layers\n",
    "\n",
    "Embeddings convert categorical variables to dense vectors:\n",
    "\n",
    "$$\\mathbf{e}_u = \\text{Embedding}(\\text{user_id}_u) \\in \\mathbb{R}^d$$\n",
    "\n",
    "$$\\mathbf{e}_i = \\text{Embedding}(\\text{item_id}_i) \\in \\mathbb{R}^d$$\n",
    "\n",
    "**Properties**: - **Dimensionality**: $d$ is typically 16-512 -\n",
    "**Initialization**: Usually random initialization with small variance -\n",
    "**Learning**: Embeddings are learned end-to-end with the model\n",
    "\n",
    "#### 2. Multi-layer Perceptrons (MLPs)\n",
    "\n",
    "MLPs capture non-linear interactions:\n",
    "\n",
    "$$\\mathbf{h}^{(l+1)} = \\text{ReLU}(\\mathbf{W}^{(l)} \\mathbf{h}^{(l)} + \\mathbf{b}^{(l)})$$\n",
    "\n",
    "where ReLU is $\\text{ReLU}(x) = \\max(0, x)$.\n",
    "\n",
    "**Advantages**: - **Non-linearity**: ReLU introduces non-linearity -\n",
    "**Sparsity**: ReLU can create sparse representations - **Gradient\n",
    "Flow**: ReLU helps with gradient flow in deep networks\n",
    "\n",
    "#### 3. Regularization Techniques\n",
    "\n",
    "**Dropout**: Randomly zeroes activations during training:\n",
    "$$\\mathbf{h}_{\\text{dropout}} = \\mathbf{h} \\odot \\mathbf{m}, \\quad \\mathbf{m} \\sim \\text{Bernoulli}(p)$$\n",
    "\n",
    "**Weight Decay**: Adds L2 regularization:\n",
    "$$\\mathcal{L}_{\\text{reg}} = \\mathcal{L} + \\lambda \\sum_{\\theta} \\|\\theta\\|_2^2$$\n",
    "\n",
    "**Batch Normalization**: Normalizes activations:\n",
    "$$\\text{BN}(\\mathbf{x}) = \\gamma \\frac{\\mathbf{x} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "### Optimization Strategies\n",
    "\n",
    "#### 1. Gradient Descent Variants\n",
    "\n",
    "**Adam Optimizer**:\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla \\mathcal{L}(\\theta_t)\n",
    "v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla \\mathcal{L}(\\theta_t))^2\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} m_t$$\n",
    "\n",
    "**RMSprop**:\n",
    "$$v_t = \\rho v_{t-1} + (1-\\rho) (\\nabla \\mathcal{L}(\\theta_t))^2\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} \\nabla \\mathcal{L}(\\theta_t)$$\n",
    "\n",
    "#### 2. Learning Rate Scheduling\n",
    "\n",
    "**Exponential Decay**:\n",
    "$$\\alpha_t = \\alpha_0 \\cdot \\gamma^t$$\n",
    "\n",
    "**Cosine Annealing**:\n",
    "$$\\alpha_t = \\alpha_{\\min} + \\frac{1}{2}(\\alpha_{\\max} - \\alpha_{\\min})(1 + \\cos(\\frac{t}{T}\\pi))$$\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "#### 1. Rating Prediction Metrics\n",
    "\n",
    "**Mean Absolute Error (MAE)**:\n",
    "$$\\text{MAE} = \\frac{1}{N} \\sum_{(u,i) \\in \\mathcal{R}} |r_{ui} - \\hat{r}_{ui}|$$\n",
    "\n",
    "**Root Mean Square Error (RMSE)**:\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{(u,i) \\in \\mathcal{R}} (r_{ui} - \\hat{r}_{ui})^2}$$\n",
    "\n",
    "#### 2. Ranking Metrics\n",
    "\n",
    "**Precision@k**:\n",
    "$$\\text{Precision@k} = \\frac{|\\text{relevant items in top-k}|}{k}$$\n",
    "\n",
    "**Recall@k**:\n",
    "$$\\text{Recall@k} = \\frac{|\\text{relevant items in top-k}|}{|\\text{total relevant items}|}$$\n",
    "\n",
    "**NDCG@k**:\n",
    "$$\\text{NDCG@k} = \\frac{\\text{DCG@k}}{\\text{IDCG@k}}$$\n",
    "\n",
    "where $\\text{DCG@k} = \\sum_{i=1}^k \\frac{2^{rel_i} - 1}{\\log_2(i+1)}$.\n",
    "\n",
    "### Theoretical Guarantees\n",
    "\n",
    "#### 1. Convergence Properties\n",
    "\n",
    "Under certain conditions, gradient descent converges to local minima:\n",
    "\n",
    "**Theorem**: If the loss function is Lipschitz continuous and the\n",
    "learning rate is sufficiently small, gradient descent converges to a\n",
    "stationary point.\n",
    "\n",
    "#### 2. Generalization Bounds\n",
    "\n",
    "**Theorem**: For a neural network with $L$ layers and $W$ parameters,\n",
    "with probability at least $1-\\delta$:\n",
    "\n",
    "$$\\mathbb{E}[\\mathcal{L}(\\hat{f})] \\leq \\hat{\\mathcal{L}}(\\hat{f}) + O\\left(\\sqrt{\\frac{W \\log(W) + \\log(1/\\delta)}{n}}\\right)$$\n",
    "\n",
    "where $\\hat{\\mathcal{L}}$ is the empirical loss and $n$ is the number of\n",
    "training samples.\n",
    "\n",
    "This theoretical foundation provides the mathematical justification for\n",
    "why deep learning can be effective for recommender systems, while also\n",
    "highlighting the importance of proper regularization and training\n",
    "procedures.\n",
    "\n",
    "## 13.7.2. Neural Collaborative Filtering (NCF)\n",
    "\n",
    "### Motivation and Intuition\n",
    "\n",
    "Neural Collaborative Filtering (NCF) was introduced to address the\n",
    "fundamental limitation of traditional matrix factorization: the\n",
    "assumption of linear interactions between user and item latent factors.\n",
    "While matrix factorization assumes\n",
    "$\\hat{r}_{ui} = \\mathbf{u}_u^T \\mathbf{v}_i$, real-world user\n",
    "preferences often exhibit complex, non-linear patterns.\n",
    "\n",
    "#### Why Neural Networks for CF?\n",
    "\n",
    "1.  **Non-linear Interactions**: Users may have complex preference\n",
    "    patterns that cannot be captured by simple dot products\n",
    "2.  **Feature Learning**: Neural networks can automatically learn\n",
    "    optimal feature representations\n",
    "3.  **Flexibility**: Can incorporate additional features beyond\n",
    "    user-item IDs\n",
    "4.  **Universal Approximation**: Can theoretically approximate any\n",
    "    continuous function\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "#### Traditional Matrix Factorization Limitation\n",
    "\n",
    "The traditional MF model assumes:\n",
    "$$\\hat{r}_{ui} = \\mathbf{u}_u^T \\mathbf{v}_i = \\sum_{k=1}^K u_{uk} v_{ik}$$\n",
    "\n",
    "This is inherently linear and cannot capture interactions like: -\n",
    "**Complementary Effects**: User likes action movies AND comedies -\n",
    "**Substitution Effects**: User prefers either action OR comedy, not\n",
    "both - **Context-dependent Preferences**: Preferences that change based\n",
    "on context\n",
    "\n",
    "#### NCF Architecture Design\n",
    "\n",
    "NCF replaces the inner product with a multi-layer neural network:\n",
    "\n",
    "$$\\hat{r}_{ui} = f(\\mathbf{u}_u, \\mathbf{v}_i) = \\sigma(\\mathbf{W}_2 \\cdot \\text{ReLU}(\\mathbf{W}_1 \\cdot [\\mathbf{u}_u; \\mathbf{v}_i] + \\mathbf{b}_1) + \\mathbf{b}_2)$$\n",
    "\n",
    "**Mathematical Components**:\n",
    "\n",
    "1.  **Embedding Layer**:\n",
    "    $$\\mathbf{u}_u = \\text{Embedding}_u(\\text{user_id}_u) \\in \\mathbb{R}^K$$\n",
    "\n",
    "    $$\\mathbf{v}_i = \\text{Embedding}_i(\\text{item_id}_i) \\in \\mathbb{R}^K$$\n",
    "\n",
    "2.  **Concatenation**:\n",
    "    $$\\mathbf{x} = [\\mathbf{u}_u; \\mathbf{v}_i] \\in \\mathbb{R}^{2K}$$\n",
    "\n",
    "3.  **Hidden Layer**:\n",
    "    $$\\mathbf{h}_1 = \\text{ReLU}(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1) \\in \\mathbb{R}^{H_1}$$\n",
    "\n",
    "4.  **Output Layer**:\n",
    "    $$\\hat{r}_{ui} = \\sigma(\\mathbf{W}_2 \\mathbf{h}_1 + \\mathbf{b}_2) \\in [0,1]$$\n",
    "\n",
    "where: - $K$ is the embedding dimension - $H_1$ is the hidden layer\n",
    "size - $\\sigma$ is the sigmoid function for output normalization\n",
    "\n",
    "#### Activation Functions\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**:\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "**Properties**: - **Non-linearity**: Introduces non-linearity to capture\n",
    "complex patterns - **Sparsity**: Can create sparse representations -\n",
    "**Gradient Flow**: Helps with gradient flow in deep networks -\n",
    "**Computational Efficiency**: Simple to compute and differentiate\n",
    "\n",
    "**Sigmoid**:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "**Properties**: - **Output Range**: Maps to $[0,1]$ for probability\n",
    "interpretation - **Smooth**: Continuous and differentiable everywhere -\n",
    "**Saturation**: Can suffer from vanishing gradients\n",
    "\n",
    "### Loss Function Design\n",
    "\n",
    "#### Binary Cross-Entropy Loss\n",
    "\n",
    "For implicit feedback (click/no-click, like/dislike):\n",
    "\n",
    "$$\\mathcal{L}_{\\text{BCE}} = -\\sum_{(u,i) \\in \\mathcal{R}^+} \\log(\\hat{r}_{ui}) - \\sum_{(u,i) \\in \\mathcal{R}^-} \\log(1 - \\hat{r}_{ui})$$\n",
    "\n",
    "where: - $\\mathcal{R}^+$ is the set of positive interactions -\n",
    "$\\mathcal{R}^-$ is the set of negative samples\n",
    "\n",
    "#### Negative Sampling Strategy\n",
    "\n",
    "Since most user-item pairs are negative, we need efficient sampling:\n",
    "\n",
    "1.  **Uniform Sampling**: Randomly sample from unobserved pairs\n",
    "    $$\\mathcal{R}^- = \\{(u,i) : (u,i) \\notin \\mathcal{R}^+\\}$$\n",
    "\n",
    "2.  **Popularity-based Sampling**: Sample based on item popularity\n",
    "    $$P(i) \\propto \\text{popularity}(i)^{\\alpha}$$\n",
    "\n",
    "3.  **Hard Negative Mining**: Sample difficult negative examples\n",
    "    $$\\mathcal{R}^- = \\{(u,i) : \\hat{r}_{ui} > \\text{threshold}\\}$$\n",
    "\n",
    "#### Mean Squared Error Loss\n",
    "\n",
    "For explicit feedback (ratings):\n",
    "\n",
    "$$\\mathcal{L}_{\\text{MSE}} = \\frac{1}{|\\mathcal{R}|} \\sum_{(u,i) \\in \\mathcal{R}} (r_{ui} - \\hat{r}_{ui})^2$$\n",
    "\n",
    "### Training Algorithm\n",
    "\n",
    "#### Forward Pass\n",
    "\n",
    "1.  **Input**: User ID $u$, Item ID $i$\n",
    "2.  **Embedding**: Look up embeddings $\\mathbf{u}_u$, $\\mathbf{v}_i$\n",
    "3.  **Concatenation**: $\\mathbf{x} = [\\mathbf{u}_u; \\mathbf{v}_i]$\n",
    "4.  **Hidden Layer**:\n",
    "    $\\mathbf{h}_1 = \\text{ReLU}(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1)$\n",
    "5.  **Output**:\n",
    "    $\\hat{r}_{ui} = \\sigma(\\mathbf{W}_2 \\mathbf{h}_1 + \\mathbf{b}_2)$\n",
    "\n",
    "#### Backward Pass\n",
    "\n",
    "**Gradient Computation**:\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_2} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{r}_{ui}} \\cdot \\frac{\\partial \\hat{r}_{ui}}{\\partial \\mathbf{W}_2}$$\n",
    "\n",
    "**Chain Rule**:\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_1} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{r}_{ui}} \\cdot \\frac{\\partial \\hat{r}_{ui}}{\\partial \\mathbf{h}_1} \\cdot \\frac{\\partial \\mathbf{h}_1}{\\partial \\mathbf{W}_1}$$\n",
    "\n",
    "#### Optimization\n",
    "\n",
    "**Adam Optimizer**:\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla \\mathcal{L}(\\theta_t)\n",
    "v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla \\mathcal{L}(\\theta_t))^2\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} m_t$$\n",
    "\n",
    "### Theoretical Analysis\n",
    "\n",
    "#### Expressiveness\n",
    "\n",
    "**Theorem**: NCF with one hidden layer can approximate any continuous\n",
    "function $f: \\mathbb{R}^{2K} \\rightarrow [0,1]$ to arbitrary precision.\n",
    "\n",
    "**Proof Sketch**: By the universal approximation theorem, a neural\n",
    "network with one hidden layer can approximate any continuous function.\n",
    "The sigmoid output ensures the range is $[0,1]$.\n",
    "\n",
    "#### Capacity vs. Traditional MF\n",
    "\n",
    "**Traditional MF**: $O(K)$ parameters per user/item **NCF**:\n",
    "$O(K + H_1 + H_1 \\cdot H_2)$ parameters total\n",
    "\n",
    "The increased capacity allows NCF to capture more complex patterns.\n",
    "\n",
    "#### Overfitting Prevention\n",
    "\n",
    "1.  **Dropout**: Randomly zero activations during training\n",
    "    $$\\mathbf{h}_{\\text{dropout}} = \\mathbf{h} \\odot \\mathbf{m}, \\quad \\mathbf{m} \\sim \\text{Bernoulli}(p)$$\n",
    "\n",
    "2.  **Weight Decay**: L2 regularization\n",
    "    $$\\mathcal{L}_{\\text{reg}} = \\mathcal{L} + \\lambda \\sum_{\\theta} \\|\\theta\\|_2^2$$\n",
    "\n",
    "3.  **Early Stopping**: Stop training when validation loss increases\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "#### Hyperparameter Tuning\n",
    "\n",
    "1.  **Embedding Dimension** $K$: Typically 16-512\n",
    "2.  **Hidden Layer Size** $H_1$: Usually 2-4x embedding dimension\n",
    "3.  **Learning Rate** $\\alpha$: Start with 0.001, use learning rate\n",
    "    scheduling\n",
    "4.  **Dropout Rate** $p$: Usually 0.1-0.5\n",
    "5.  **Batch Size**: 32-256, depending on memory constraints\n",
    "\n",
    "#### Initialization Strategies\n",
    "\n",
    "1.  **Xavier/Glorot Initialization**:\n",
    "    $$W_{ij} \\sim \\mathcal{N}(0, \\frac{2}{n_{\\text{in}} + n_{\\text{out}}})$$\n",
    "\n",
    "2.  **He Initialization** (for ReLU):\n",
    "    $$W_{ij} \\sim \\mathcal{N}(0, \\frac{2}{n_{\\text{in}}})$$\n",
    "\n",
    "#### Training Tips\n",
    "\n",
    "1.  **Data Preprocessing**: Normalize features, handle missing values\n",
    "2.  **Validation Strategy**: Use time-based split for temporal data\n",
    "3.  **Evaluation Metrics**: Use ranking metrics for implicit feedback\n",
    "4.  **Model Selection**: Cross-validation with multiple random seeds\n",
    "\n",
    "### Comparison with Traditional Methods\n",
    "\n",
    "| Aspect               | Matrix Factorization | NCF                        |\n",
    "|----------------------|----------------------|----------------------------|\n",
    "| **Linearity**        | Linear interactions  | Non-linear interactions    |\n",
    "| **Expressiveness**   | Limited              | High                       |\n",
    "| **Parameters**       | $O(K \\cdot (N+M))$   | $O(K \\cdot (N+M) + H_1^2)$ |\n",
    "| **Training Time**    | Fast                 | Slower                     |\n",
    "| **Interpretability** | High                 | Low                        |\n",
    "| **Cold Start**       | Poor                 | Better with features       |\n",
    "\n",
    "This detailed mathematical foundation provides the theoretical\n",
    "understanding needed to implement and optimize NCF models effectively.\n",
    "\n",
    "## 13.7.3. Wide & Deep Learning\n",
    "\n",
    "### Motivation and Problem Statement\n",
    "\n",
    "Wide & Deep Learning was developed by Google to address the fundamental\n",
    "trade-off between **memorization** and **generalization** in recommender\n",
    "systems. Traditional approaches often struggle to balance these two\n",
    "objectives:\n",
    "\n",
    "- **Memorization**: Learning frequent co-occurrence patterns from\n",
    "  historical data\n",
    "- **Generalization**: Discovering unseen feature combinations for better\n",
    "  generalization\n",
    "\n",
    "#### The Memorization vs. Generalization Trade-off\n",
    "\n",
    "**Memorization** captures frequent patterns in training data: - User A\n",
    "who watched action movies also watches thrillers - Item B is frequently\n",
    "purchased with item C - Specific feature combinations that appear often\n",
    "\n",
    "**Generalization** discovers new patterns: - Users who like sci-fi also\n",
    "tend to like documentaries - Cross-category preferences (e.g., tech\n",
    "enthusiasts liking cooking shows) - Unseen feature combinations\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "#### Problem Formulation\n",
    "\n",
    "Given input features\n",
    "$\\mathbf{x} = [\\mathbf{x}_{\\text{wide}}, \\mathbf{x}_{\\text{deep}}]$,\n",
    "predict the probability of a positive interaction:\n",
    "\n",
    "$$P(y = 1 | \\mathbf{x}) = \\sigma(\\mathbf{w}_{\\text{wide}}^T \\phi_{\\text{wide}}(\\mathbf{x}) + \\mathbf{w}_{\\text{deep}}^T \\phi_{\\text{deep}}(\\mathbf{x}) + b)$$\n",
    "\n",
    "where: - $\\mathbf{x}_{\\text{wide}}$: Wide features (sparse,\n",
    "categorical) - $\\mathbf{x}_{\\text{deep}}$: Deep features (dense,\n",
    "continuous) - $\\phi_{\\text{wide}}$: Wide feature transformation -\n",
    "$\\phi_{\\text{deep}}$: Deep feature transformation - $\\sigma$: Sigmoid\n",
    "activation function\n",
    "\n",
    "#### Wide Component: Memorization\n",
    "\n",
    "The wide component captures memorization through linear models and\n",
    "cross-product features:\n",
    "\n",
    "$$\\phi_{\\text{wide}}(\\mathbf{x}) = [\\mathbf{x}_{\\text{wide}}, \\text{cross}(\\mathbf{x}_{\\text{wide}})]$$\n",
    "\n",
    "**Cross-Product Features**:\n",
    "$$\\text{cross}(\\mathbf{x}) = \\prod_{i=1}^k x_i^{c_{ki}}$$\n",
    "\n",
    "where $c_{ki} \\in \\{0,1\\}$ indicates whether feature $i$ is included in\n",
    "cross-product $k$.\n",
    "\n",
    "**Mathematical Properties**: - **Sparsity**: Most cross-products are\n",
    "zero, enabling efficient computation - **Interpretability**: Each\n",
    "cross-product represents a specific feature combination -\n",
    "**Memorization**: Captures exact patterns from training data\n",
    "\n",
    "**Example**: For features $[user\\_id, item\\_id, category]$:\n",
    "$$\\text{cross}([u_1, i_2, c_3]) = [u_1 \\cdot i_2, u_1 \\cdot c_3, i_2 \\cdot c_3, u_1 \\cdot i_2 \\cdot c_3]$$\n",
    "\n",
    "#### Deep Component: Generalization\n",
    "\n",
    "The deep component learns distributed representations through embeddings\n",
    "and neural networks:\n",
    "\n",
    "$$\\phi_{\\text{deep}}(\\mathbf{x}) = \\text{MLP}(\\text{embed}(\\mathbf{x}_{\\text{deep}}))$$\n",
    "\n",
    "**Embedding Layer**:\n",
    "$$\\text{embed}(\\mathbf{x}_{\\text{deep}}) = [\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n]$$\n",
    "\n",
    "where $\\mathbf{e}_i = \\text{Embedding}_i(x_i)$ for categorical feature\n",
    "$i$.\n",
    "\n",
    "**Multi-Layer Perceptron**:\n",
    "$$\\mathbf{h}^{(l+1)} = \\text{ReLU}(\\mathbf{W}^{(l)} \\mathbf{h}^{(l)} + \\mathbf{b}^{(l)})$$\n",
    "\n",
    "**Mathematical Properties**: - **Distributed Representations**:\n",
    "Embeddings capture semantic relationships - **Non-linearity**: ReLU\n",
    "enables complex pattern learning - **Generalization**: Can generalize to\n",
    "unseen feature combinations\n",
    "\n",
    "### Detailed Architecture\n",
    "\n",
    "#### 1. Input Processing\n",
    "\n",
    "**Wide Features** (sparse, categorical):\n",
    "$$\\mathbf{x}_{\\text{wide}} = [\\text{user_id}, \\text{item_id}, \\text{category}, \\text{time_slot}]$$\n",
    "\n",
    "**Deep Features** (dense, continuous):\n",
    "$$\\mathbf{x}_{\\text{deep}} = [\\text{user_embedding}, \\text{item_embedding}, \\text{context_features}]$$\n",
    "\n",
    "#### 2. Wide Component Implementation\n",
    "\n",
    "**Linear Model**:\n",
    "$$f_{\\text{wide}}(\\mathbf{x}) = \\mathbf{w}_{\\text{wide}}^T \\phi_{\\text{wide}}(\\mathbf{x}) + b_{\\text{wide}}$$\n",
    "\n",
    "**Cross-Product Generation**:\n",
    "$$\\phi_{\\text{cross}}(\\mathbf{x}) = \\prod_{i \\in S} x_i$$\n",
    "\n",
    "where $S$ is a subset of feature indices.\n",
    "\n",
    "**Sparse Implementation**:\n",
    "$$\\phi_{\\text{wide}}(\\mathbf{x}) = \\text{OneHot}(\\mathbf{x}) + \\text{CrossProducts}(\\mathbf{x})$$\n",
    "\n",
    "#### 3. Deep Component Implementation\n",
    "\n",
    "**Embedding Layer**:\n",
    "$$\\mathbf{e}_i = \\mathbf{E}_i \\mathbf{x}_i$$\n",
    "\n",
    "where $\\mathbf{E}_i$ is the embedding matrix for feature $i$.\n",
    "\n",
    "**Concatenation**:\n",
    "$$\\mathbf{h}^{(0)} = [\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n]$$\n",
    "\n",
    "**Hidden Layers**:\n",
    "$$\\mathbf{h}^{(l+1)} = \\text{ReLU}(\\mathbf{W}^{(l)} \\mathbf{h}^{(l)} + \\mathbf{b}^{(l)})$$\n",
    "\n",
    "**Output**:\n",
    "$$f_{\\text{deep}}(\\mathbf{x}) = \\mathbf{w}_{\\text{deep}}^T \\mathbf{h}^{(L)} + b_{\\text{deep}}$$\n",
    "\n",
    "#### 4. Joint Training\n",
    "\n",
    "**Combined Output**:\n",
    "$$\\hat{y} = \\sigma(f_{\\text{wide}}(\\mathbf{x}) + f_{\\text{deep}}(\\mathbf{x}))$$\n",
    "\n",
    "**Loss Function**:\n",
    "$$\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^N [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]$$\n",
    "\n",
    "### Training Algorithm\n",
    "\n",
    "#### 1. Forward Pass\n",
    "\n",
    "1.  **Wide Component**:\n",
    "    $$f_{\\text{wide}} = \\mathbf{w}_{\\text{wide}}^T \\phi_{\\text{wide}}(\\mathbf{x}) + b_{\\text{wide}}$$\n",
    "\n",
    "2.  **Deep Component**:\n",
    "    $$\\mathbf{h}^{(0)} = \\text{embed}(\\mathbf{x}_{\\text{deep}})\n",
    "    \\mathbf{h}^{(l+1)} = \\text{ReLU}(\\mathbf{W}^{(l)} \\mathbf{h}^{(l)} + \\mathbf{b}^{(l)})\n",
    "    f_{\\text{deep}} = \\mathbf{w}_{\\text{deep}}^T \\mathbf{h}^{(L)} + b_{\\text{deep}}$$\n",
    "\n",
    "3.  **Combined Prediction**:\n",
    "    $$\\hat{y} = \\sigma(f_{\\text{wide}} + f_{\\text{deep}})$$\n",
    "\n",
    "#### 2. Backward Pass\n",
    "\n",
    "**Gradient for Wide Component**:\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}_{\\text{wide}}} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial f_{\\text{wide}}} \\cdot \\phi_{\\text{wide}}(\\mathbf{x})$$\n",
    "\n",
    "**Gradient for Deep Component**:\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}^{(l+1)}} \\cdot \\frac{\\partial \\mathbf{h}^{(l+1)}}{\\partial \\mathbf{W}^{(l)}}$$\n",
    "\n",
    "#### 3. Optimization\n",
    "\n",
    "**FTRL (Follow-the-Regularized-Leader) for Wide**:\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\frac{\\alpha_t}{\\sqrt{\\sum_{s=1}^t g_s^2}} g_t$$\n",
    "\n",
    "**AdaGrad for Deep**:\n",
    "$$\\mathbf{W}_{t+1} = \\mathbf{W}_t - \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}} \\nabla \\mathcal{L}(\\mathbf{W}_t)$$\n",
    "\n",
    "### Theoretical Analysis\n",
    "\n",
    "#### 1. Memorization Capacity\n",
    "\n",
    "**Theorem**: The wide component can memorize any binary pattern with\n",
    "sufficient cross-products.\n",
    "\n",
    "**Proof Sketch**: For any binary pattern $\\mathbf{x}$, there exists a\n",
    "cross-product that is 1 only for that pattern.\n",
    "\n",
    "#### 2. Generalization Bounds\n",
    "\n",
    "**Theorem**: For a deep network with $L$ layers and $W$ parameters:\n",
    "$$\\mathbb{E}[\\mathcal{L}(\\hat{f})] \\leq \\hat{\\mathcal{L}}(\\hat{f}) + O\\left(\\sqrt{\\frac{W \\log(W)}{n}}\\right)$$\n",
    "\n",
    "#### 3. Joint Training Benefits\n",
    "\n",
    "**Theorem**: Joint training of wide and deep components provides better\n",
    "generalization than training them separately.\n",
    "\n",
    "**Intuition**: The wide component provides a good initialization for the\n",
    "deep component, while the deep component helps regularize the wide\n",
    "component.\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "#### 1. Feature Engineering\n",
    "\n",
    "**Wide Features**: - User ID, Item ID - Cross-product features (user_id\n",
    "× item_id) - Contextual features (time, location) - Categorical features\n",
    "with high cardinality\n",
    "\n",
    "**Deep Features**: - Continuous features (age, price) - Embeddings of\n",
    "categorical features - Pre-trained embeddings - Contextual embeddings\n",
    "\n",
    "#### 2. Hyperparameter Tuning\n",
    "\n",
    "**Wide Component**: - Cross-product degree: 2-3 - Regularization\n",
    "strength: $\\lambda_{\\text{wide}} = 0.01-0.1$\n",
    "\n",
    "**Deep Component**: - Embedding dimensions: 8-64 - Hidden layer sizes:\n",
    "100-1000 - Dropout rate: 0.1-0.5 - Learning rate: 0.001-0.01\n",
    "\n",
    "#### 3. Training Strategies\n",
    "\n",
    "**Joint Training**: - Train wide and deep components together - Use\n",
    "different optimizers for each component - Monitor both components’\n",
    "performance\n",
    "\n",
    "**Progressive Training**: - Train wide component first - Freeze wide\n",
    "component - Train deep component - Fine-tune both components\n",
    "\n",
    "### Comparison with Other Methods\n",
    "\n",
    "| Aspect                  | Linear Models | Deep Models | Wide & Deep |\n",
    "|-------------------------|---------------|-------------|-------------|\n",
    "| **Memorization**        | High          | Low         | High        |\n",
    "| **Generalization**      | Low           | High        | High        |\n",
    "| **Interpretability**    | High          | Low         | Medium      |\n",
    "| **Training Speed**      | Fast          | Slow        | Medium      |\n",
    "| **Feature Engineering** | Required      | Automatic   | Hybrid      |\n",
    "\n",
    "### Advantages and Limitations\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "1.  **Balanced Approach**: Combines memorization and generalization\n",
    "2.  **Interpretability**: Wide component provides interpretable features\n",
    "3.  **Scalability**: Can handle large-scale production systems\n",
    "4.  **Flexibility**: Can incorporate various feature types\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "1.  **Feature Engineering**: Still requires manual feature engineering\n",
    "    for wide component\n",
    "2.  **Hyperparameter Tuning**: More parameters to tune\n",
    "3.  **Computational Cost**: More expensive than simple models\n",
    "4.  **Interpretability**: Deep component remains a black box\n",
    "\n",
    "This comprehensive mathematical foundation provides the theoretical\n",
    "understanding and practical guidance needed to implement Wide & Deep\n",
    "models effectively in production recommender systems.\n",
    "\n",
    "## 13.7.4. Deep Matrix Factorization\n",
    "\n",
    "### Motivation and Problem Statement\n",
    "\n",
    "Deep Matrix Factorization extends traditional matrix factorization by\n",
    "incorporating neural networks to capture both linear and non-linear\n",
    "interactions between users and items. The key insight is that while\n",
    "traditional MF captures linear interactions through dot products,\n",
    "real-world user preferences often exhibit complex, non-linear patterns.\n",
    "\n",
    "#### Why Deep Matrix Factorization?\n",
    "\n",
    "1.  **Linear + Non-linear Modeling**: Combines the interpretability of\n",
    "    linear models with the expressiveness of neural networks\n",
    "2.  **Hybrid Approach**: Leverages both collaborative filtering and\n",
    "    content-based information\n",
    "3.  **Flexible Architecture**: Can incorporate various types of features\n",
    "    and interactions\n",
    "4.  **Theoretical Foundation**: Builds upon well-established matrix\n",
    "    factorization theory\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "#### Traditional Matrix Factorization Revisited\n",
    "\n",
    "The traditional MF model assumes:\n",
    "$$\\hat{r}_{ui} = \\mathbf{u}_u^T \\mathbf{v}_i = \\sum_{k=1}^K u_{uk} v_{ik}$$\n",
    "\n",
    "This can be viewed as a special case of a more general interaction\n",
    "function:\n",
    "$$\\hat{r}_{ui} = f(\\mathbf{u}_u, \\mathbf{v}_i)$$\n",
    "\n",
    "where $f$ is the interaction function.\n",
    "\n",
    "#### Generalized Matrix Factorization (GMF)\n",
    "\n",
    "GMF extends traditional MF by allowing non-linear transformations:\n",
    "\n",
    "$$\\phi_{\\text{GMF}}(\\mathbf{u}_u, \\mathbf{v}_i) = \\mathbf{u}_u \\odot \\mathbf{v}_i$$\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication (Hadamard product).\n",
    "\n",
    "**Mathematical Properties**: - **Element-wise Interaction**: Each latent\n",
    "dimension interacts independently - **Non-linearity**: Can capture\n",
    "complex interaction patterns - **Interpretability**: Each dimension\n",
    "represents a specific aspect of preference\n",
    "\n",
    "#### Multi-Layer Perceptron (MLP) Component\n",
    "\n",
    "The MLP component learns non-linear interactions through neural\n",
    "networks:\n",
    "\n",
    "$$\\phi_{\\text{MLP}}(\\mathbf{u}_u, \\mathbf{v}_i) = \\text{MLP}([\\mathbf{u}_u; \\mathbf{v}_i])$$\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "$$\\mathbf{h}^{(0)} = [\\mathbf{u}_u; \\mathbf{v}_i]\n",
    "\\mathbf{h}^{(l+1)} = \\text{ReLU}(\\mathbf{W}^{(l)} \\mathbf{h}^{(l)} + \\mathbf{b}^{(l)})\n",
    "\\phi_{\\text{MLP}} = \\mathbf{h}^{(L)}$$\n",
    "\n",
    "where $L$ is the number of layers.\n",
    "\n",
    "### Neural Matrix Factorization (NeuMF)\n",
    "\n",
    "#### Architecture Design\n",
    "\n",
    "NeuMF combines GMF and MLP components through a neural fusion layer:\n",
    "\n",
    "$$\\hat{r}_{ui} = \\sigma(\\mathbf{h}^T \\cdot [\\phi_{\\text{GMF}}(\\mathbf{u}_u, \\mathbf{v}_i); \\phi_{\\text{MLP}}(\\mathbf{u}_u, \\mathbf{v}_i)])$$\n",
    "\n",
    "**Mathematical Components**:\n",
    "\n",
    "1.  **GMF Component**:\n",
    "    $$\\phi_{\\text{GMF}}(\\mathbf{u}_u, \\mathbf{v}_i) = \\mathbf{u}_u \\odot \\mathbf{v}_i \\in \\mathbb{R}^K$$\n",
    "\n",
    "2.  **MLP Component**:\n",
    "    $$\\phi_{\\text{MLP}}(\\mathbf{u}_u, \\mathbf{v}_i) = \\text{MLP}([\\mathbf{u}_u; \\mathbf{v}_i]) \\in \\mathbb{R}^{H_L}$$\n",
    "\n",
    "3.  **Fusion Layer**:\n",
    "    $$\\mathbf{z} = [\\phi_{\\text{GMF}}; \\phi_{\\text{MLP}}] \\in \\mathbb{R}^{K + H_L}$$\n",
    "\n",
    "4.  **Output Layer**:\n",
    "    $$\\hat{r}_{ui} = \\sigma(\\mathbf{h}^T \\mathbf{z} + b) \\in [0,1]$$\n",
    "\n",
    "#### Parameter Sharing Strategy\n",
    "\n",
    "**Shared Embeddings**: Both GMF and MLP can share the same embedding\n",
    "matrices:\n",
    "$$\\mathbf{u}_u^{\\text{GMF}} = \\mathbf{u}_u^{\\text{MLP}} = \\mathbf{u}_u\n",
    "\\mathbf{v}_i^{\\text{GMF}} = \\mathbf{v}_i^{\\text{MLP}} = \\mathbf{v}_i$$\n",
    "\n",
    "**Separate Embeddings**: Each component has its own embeddings:\n",
    "$$\\mathbf{u}_u^{\\text{GMF}} \\neq \\mathbf{u}_u^{\\text{MLP}}\n",
    "\\mathbf{v}_i^{\\text{GMF}} \\neq \\mathbf{v}_i^{\\text{MLP}}$$\n",
    "\n",
    "### Training Strategy\n",
    "\n",
    "#### 1. Pre-training Phase\n",
    "\n",
    "**GMF Pre-training**:\n",
    "$$\\mathcal{L}_{\\text{GMF}} = \\frac{1}{|\\mathcal{R}|} \\sum_{(u,i) \\in \\mathcal{R}} (r_{ui} - \\hat{r}_{ui}^{\\text{GMF}})^2$$\n",
    "\n",
    "where\n",
    "$\\hat{r}_{ui}^{\\text{GMF}} = \\sigma(\\mathbf{h}_{\\text{GMF}}^T (\\mathbf{u}_u \\odot \\mathbf{v}_i))$.\n",
    "\n",
    "**MLP Pre-training**:\n",
    "$$\\mathcal{L}_{\\text{MLP}} = \\frac{1}{|\\mathcal{R}|} \\sum_{(u,i) \\in \\mathcal{R}} (r_{ui} - \\hat{r}_{ui}^{\\text{MLP}})^2$$\n",
    "\n",
    "where\n",
    "$\\hat{r}_{ui}^{\\text{MLP}} = \\sigma(\\mathbf{h}_{\\text{MLP}}^T \\phi_{\\text{MLP}}(\\mathbf{u}_u, \\mathbf{v}_i))$.\n",
    "\n",
    "#### 2. Fine-tuning Phase\n",
    "\n",
    "**Joint Training**:\n",
    "$$\\mathcal{L}_{\\text{NeuMF}} = \\frac{1}{|\\mathcal{R}|} \\sum_{(u,i) \\in \\mathcal{R}} (r_{ui} - \\hat{r}_{ui})^2 + \\lambda \\sum_{\\theta} \\|\\theta\\|_2^2$$\n",
    "\n",
    "where $\\hat{r}_{ui}$ is the NeuMF prediction.\n",
    "\n",
    "#### 3. Ensemble Strategy\n",
    "\n",
    "**Weighted Ensemble**:\n",
    "$$\\hat{r}_{ui} = \\alpha \\cdot \\hat{r}_{ui}^{\\text{GMF}} + (1-\\alpha) \\cdot \\hat{r}_{ui}^{\\text{MLP}}$$\n",
    "\n",
    "where $\\alpha$ is learned during training.\n",
    "\n",
    "### Mathematical Analysis\n",
    "\n",
    "#### 1. Expressiveness\n",
    "\n",
    "**Theorem**: NeuMF can approximate any continuous function\n",
    "$f: \\mathbb{R}^{2K} \\rightarrow [0,1]$ to arbitrary precision.\n",
    "\n",
    "**Proof Sketch**: - GMF component captures linear interactions - MLP\n",
    "component captures non-linear interactions - Fusion layer combines both\n",
    "types of interactions - Universal approximation theorem applies to the\n",
    "overall architecture\n",
    "\n",
    "#### 2. Capacity Analysis\n",
    "\n",
    "**Parameter Count**: - **GMF**: $O(K \\cdot (N + M) + K)$ parameters -\n",
    "**MLP**: $O(K \\cdot (N + M) + \\sum_{l=1}^L H_l^2)$ parameters -\n",
    "**NeuMF**: $O(K \\cdot (N + M) + \\sum_{l=1}^L H_l^2 + K + H_L)$\n",
    "parameters\n",
    "\n",
    "where $N$ and $M$ are the number of users and items, respectively.\n",
    "\n",
    "#### 3. Convergence Properties\n",
    "\n",
    "**Theorem**: Under certain conditions, NeuMF training converges to a\n",
    "local minimum.\n",
    "\n",
    "**Conditions**: - Loss function is Lipschitz continuous - Learning rate\n",
    "is sufficiently small - Gradients are bounded\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "#### 1. Embedding Initialization\n",
    "\n",
    "**Xavier Initialization**:\n",
    "$$\\mathbf{u}_u \\sim \\mathcal{N}(0, \\frac{2}{K})\n",
    "\\mathbf{v}_i \\sim \\mathcal{N}(0, \\frac{2}{K})$$\n",
    "\n",
    "**Pre-trained Initialization**: Use embeddings from traditional MF as\n",
    "initialization.\n",
    "\n",
    "#### 2. Activation Functions\n",
    "\n",
    "**ReLU for Hidden Layers**:\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "**Sigmoid for Output**:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "#### 3. Regularization\n",
    "\n",
    "**Dropout**:\n",
    "$$\\mathbf{h}_{\\text{dropout}}^{(l)} = \\mathbf{h}^{(l)} \\odot \\mathbf{m}^{(l)}$$\n",
    "\n",
    "where $\\mathbf{m}^{(l)} \\sim \\text{Bernoulli}(p_l)$.\n",
    "\n",
    "**Weight Decay**:\n",
    "$$\\mathcal{L}_{\\text{reg}} = \\mathcal{L} + \\lambda \\sum_{\\theta} \\|\\theta\\|_2^2$$\n",
    "\n",
    "### Advanced Variants\n",
    "\n",
    "#### 1. DeepFM\n",
    "\n",
    "DeepFM extends NeuMF by incorporating factorization machines:\n",
    "\n",
    "$$\\hat{r}_{ui} = \\sigma(\\text{FM}(\\mathbf{x}) + \\text{Deep}(\\mathbf{x}))$$\n",
    "\n",
    "where $\\text{FM}(\\mathbf{x})$ is the factorization machine component.\n",
    "\n",
    "#### 2. xDeepFM\n",
    "\n",
    "xDeepFM uses compressed interaction network (CIN):\n",
    "\n",
    "$$\\mathbf{X}^{(k)} = \\text{CIN}(\\mathbf{X}^{(k-1)}, \\mathbf{X}^{(0)})$$\n",
    "\n",
    "where CIN captures high-order feature interactions.\n",
    "\n",
    "#### 3. AutoInt\n",
    "\n",
    "AutoInt uses self-attention mechanisms:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "#### 1. Hyperparameter Tuning\n",
    "\n",
    "**Architecture**: - Embedding dimension: $K = 8-64$ - MLP layers:\n",
    "$[64, 32, 16]$ or $[128, 64, 32]$ - Dropout rate: $p = 0.1-0.5$\n",
    "\n",
    "**Training**: - Learning rate: $\\alpha = 0.001-0.01$ - Batch size:\n",
    "$B = 32-256$ - Regularization: $\\lambda = 0.01-0.1$\n",
    "\n",
    "#### 2. Training Strategies\n",
    "\n",
    "**Progressive Training**: 1. Train GMF component 2. Train MLP component\n",
    "3. Joint fine-tuning 4. Ensemble if needed\n",
    "\n",
    "**Curriculum Learning**: 1. Start with simple interactions 2. Gradually\n",
    "increase complexity 3. Add regularization as training progresses\n",
    "\n",
    "#### 3. Evaluation Metrics\n",
    "\n",
    "**Rating Prediction**: - MAE:\n",
    "$\\text{MAE} = \\frac{1}{N} \\sum_{(u,i)} |r_{ui} - \\hat{r}_{ui}|$ - RMSE:\n",
    "$\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{(u,i)} (r_{ui} - \\hat{r}_{ui})^2}$\n",
    "\n",
    "**Ranking**: - NDCG@k:\n",
    "$\\text{NDCG@k} = \\frac{\\text{DCG@k}}{\\text{IDCG@k}}$ - HR@k:\n",
    "$\\text{HR@k} = \\frac{|\\text{relevant items in top-k}|}{|\\text{total relevant items}|}$\n",
    "\n",
    "### Comparison with Other Methods\n",
    "\n",
    "| Aspect               | Traditional MF | NeuMF  | Wide & Deep |\n",
    "|----------------------|----------------|--------|-------------|\n",
    "| **Linearity**        | Linear         | Hybrid | Hybrid      |\n",
    "| **Expressiveness**   | Low            | High   | High        |\n",
    "| **Interpretability** | High           | Medium | Medium      |\n",
    "| **Training Time**    | Fast           | Medium | Slow        |\n",
    "| **Memory Usage**     | Low            | Medium | High        |\n",
    "\n",
    "### Theoretical Guarantees\n",
    "\n",
    "#### 1. Approximation Power\n",
    "\n",
    "**Theorem**: NeuMF can approximate any continuous rating function to\n",
    "arbitrary precision.\n",
    "\n",
    "#### 2. Generalization Bounds\n",
    "\n",
    "**Theorem**: With probability at least $1-\\delta$:\n",
    "$$\\mathbb{E}[\\mathcal{L}(\\hat{f})] \\leq \\hat{\\mathcal{L}}(\\hat{f}) + O\\left(\\sqrt{\\frac{W \\log(W) + \\log(1/\\delta)}{n}}\\right)$$\n",
    "\n",
    "where $W$ is the number of parameters and $n$ is the number of training\n",
    "samples.\n",
    "\n",
    "#### 3. Convergence Analysis\n",
    "\n",
    "**Theorem**: Under appropriate conditions, NeuMF training converges to a\n",
    "stationary point.\n",
    "\n",
    "This comprehensive mathematical foundation provides the theoretical\n",
    "understanding and practical guidance needed to implement and optimize\n",
    "deep matrix factorization models effectively.\n",
    "\n",
    "## 13.7.5. Implementation\n",
    "\n",
    "### Python Implementation: Deep Recommender Systems\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class RatingDataset(Dataset):\n",
    "    \"\"\"Dataset for rating prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, ratings_df, user_col='user_id', item_col='item_id', rating_col='rating'):\n",
    "        self.ratings_df = ratings_df\n",
    "        \n",
    "        # Create mappings\n",
    "        self.user_mapping = {user: idx for idx, user in enumerate(ratings_df[user_col].unique())}\n",
    "        self.item_mapping = {item: idx for idx, item in enumerate(ratings_df[item_col].unique())}\n",
    "        \n",
    "        # Convert to indices\n",
    "        self.user_indices = torch.tensor([\n",
    "            self.user_mapping[user] for user in ratings_df[user_col]\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        self.item_indices = torch.tensor([\n",
    "            self.item_mapping[item] for item in ratings_df[item_col]\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        self.ratings = torch.tensor(ratings_df[rating_col].values, dtype=torch.float)\n",
    "        \n",
    "        self.n_users = len(self.user_mapping)\n",
    "        self.n_items = len(self.item_mapping)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'user_idx': self.user_indices[idx],\n",
    "            'item_idx': self.item_indices[idx],\n",
    "            'rating': self.ratings[idx]\n",
    "        }\n",
    "\n",
    "class NCF(nn.Module):\n",
    "    \"\"\"Neural Collaborative Filtering\"\"\"\n",
    "    \n",
    "    def __init__(self, n_users, n_items, n_factors=10, layers=[20, 10], dropout=0.1):\n",
    "        super(NCF, self).__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.item_embedding = nn.Embedding(n_items, n_factors)\n",
    "        \n",
    "        # MLP layers\n",
    "        self.mlp_layers = []\n",
    "        input_size = 2 * n_factors\n",
    "        \n",
    "        for layer_size in layers:\n",
    "            self.mlp_layers.extend([\n",
    "                nn.Linear(input_size, layer_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            input_size = layer_size\n",
    "        \n",
    "        self.mlp = nn.Sequential(*self.mlp_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(layers[-1], 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, std=0.01)\n",
    "    \n",
    "    def forward(self, user_idx, item_idx):\n",
    "        # Get embeddings\n",
    "        user_embed = self.user_embedding(user_idx)\n",
    "        item_embed = self.item_embedding(item_idx)\n",
    "        \n",
    "        # Concatenate\n",
    "        concat = torch.cat([user_embed, item_embed], dim=1)\n",
    "        \n",
    "        # MLP\n",
    "        mlp_output = self.mlp(concat)\n",
    "        \n",
    "        # Output\n",
    "        output = self.output_layer(mlp_output)\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "class WideAndDeep(nn.Module):\n",
    "    \"\"\"Wide & Deep Learning Model\"\"\"\n",
    "    \n",
    "    def __init__(self, n_users, n_items, n_factors=10, deep_layers=[20, 10], dropout=0.1):\n",
    "        super(WideAndDeep, self).__init__()\n",
    "        \n",
    "        # Wide component (linear)\n",
    "        self.wide_user_embedding = nn.Embedding(n_users, 1)\n",
    "        self.wide_item_embedding = nn.Embedding(n_items, 1)\n",
    "        \n",
    "        # Deep component\n",
    "        self.deep_user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.deep_item_embedding = nn.Embedding(n_items, n_factors)\n",
    "        \n",
    "        # Deep MLP\n",
    "        self.deep_layers = []\n",
    "        input_size = 2 * n_factors\n",
    "        \n",
    "        for layer_size in deep_layers:\n",
    "            self.deep_layers.extend([\n",
    "                nn.Linear(input_size, layer_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            input_size = layer_size\n",
    "        \n",
    "        self.deep_mlp = nn.Sequential(*self.deep_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(deep_layers[-1] + 2, 1)  # +2 for wide features\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, std=0.01)\n",
    "    \n",
    "    def forward(self, user_idx, item_idx):\n",
    "        # Wide component\n",
    "        wide_user = self.wide_user_embedding(user_idx).squeeze()\n",
    "        wide_item = self.wide_item_embedding(item_idx).squeeze()\n",
    "        wide_features = torch.stack([wide_user, wide_item], dim=1)\n",
    "        \n",
    "        # Deep component\n",
    "        deep_user = self.deep_user_embedding(user_idx)\n",
    "        deep_item = self.deep_item_embedding(item_idx)\n",
    "        deep_concat = torch.cat([deep_user, deep_item], dim=1)\n",
    "        deep_output = self.deep_mlp(deep_concat)\n",
    "        \n",
    "        # Combine wide and deep\n",
    "        combined = torch.cat([wide_features, deep_output], dim=1)\n",
    "        output = self.output_layer(combined)\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    \"\"\"Neural Matrix Factorization\"\"\"\n",
    "    \n",
    "    def __init__(self, n_users, n_items, n_factors=10, mlp_layers=[20, 10], dropout=0.1):\n",
    "        super(NeuMF, self).__init__()\n",
    "        \n",
    "        # GMF embeddings\n",
    "        self.gmf_user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.gmf_item_embedding = nn.Embedding(n_items, n_factors)\n",
    "        \n",
    "        # MLP embeddings\n",
    "        self.mlp_user_embedding = nn.Embedding(n_users, n_factors)\n",
    "        self.mlp_item_embedding = nn.Embedding(n_items, n_factors)\n",
    "        \n",
    "        # MLP layers\n",
    "        self.mlp_layers = []\n",
    "        input_size = 2 * n_factors\n",
    "        \n",
    "        for layer_size in mlp_layers:\n",
    "            self.mlp_layers.extend([\n",
    "                nn.Linear(input_size, layer_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            input_size = layer_size\n",
    "        \n",
    "        self.mlp = nn.Sequential(*self.mlp_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(n_factors + mlp_layers[-1], 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, std=0.01)\n",
    "    \n",
    "    def forward(self, user_idx, item_idx):\n",
    "        # GMF component\n",
    "        gmf_user = self.gmf_user_embedding(user_idx)\n",
    "        gmf_item = self.gmf_item_embedding(item_idx)\n",
    "        gmf_output = gmf_user * gmf_item  # Element-wise product\n",
    "        \n",
    "        # MLP component\n",
    "        mlp_user = self.mlp_user_embedding(user_idx)\n",
    "        mlp_item = self.mlp_item_embedding(item_idx)\n",
    "        mlp_concat = torch.cat([mlp_user, mlp_item], dim=1)\n",
    "        mlp_output = self.mlp(mlp_concat)\n",
    "        \n",
    "        # Combine\n",
    "        combined = torch.cat([gmf_output, mlp_output], dim=1)\n",
    "        output = self.output_layer(combined)\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, n_epochs=50, learning_rate=0.001, device='cpu'):\n",
    "    \"\"\"Train a deep recommendation model\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            user_idx = batch['user_idx'].to(device)\n",
    "            item_idx = batch['item_idx'].to(device)\n",
    "            ratings = batch['rating'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(user_idx, item_idx)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                user_idx = batch['user_idx'].to(device)\n",
    "                item_idx = batch['item_idx'].to(device)\n",
    "                ratings = batch['rating'].to(device)\n",
    "                \n",
    "                predictions = model(user_idx, item_idx)\n",
    "                loss = criterion(predictions, ratings)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            user_idx = batch['user_idx'].to(device)\n",
    "            item_idx = batch['item_idx'].to(device)\n",
    "            ratings = batch['rating'].to(device)\n",
    "            \n",
    "            preds = model(user_idx, item_idx)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            actuals.extend(ratings.cpu().numpy())\n",
    "    \n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    \n",
    "    return mae, rmse, predictions, actuals\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_users = 500\n",
    "n_items = 300\n",
    "n_ratings = 3000\n",
    "\n",
    "# Create synthetic ratings with non-linear patterns\n",
    "ratings_data = []\n",
    "for user_id in range(n_users):\n",
    "    n_user_ratings = np.random.randint(5, 20)\n",
    "    rated_items = np.random.choice(n_items, n_user_ratings, replace=False)\n",
    "    \n",
    "    for item_id in rated_items:\n",
    "        # Create non-linear patterns\n",
    "        user_factor = np.random.normal(0, 1)\n",
    "        item_factor = np.random.normal(0, 1)\n",
    "        \n",
    "        # Non-linear interaction\n",
    "        interaction = np.sin(user_factor) * np.cos(item_factor) + user_factor * item_factor\n",
    "        \n",
    "        # Add noise and convert to rating\n",
    "        rating = max(1, min(5, 3 + interaction + np.random.normal(0, 0.3)))\n",
    "        ratings_data.append({\n",
    "            'user_id': user_id,\n",
    "            'item_id': item_id,\n",
    "            'rating': rating\n",
    "        })\n",
    "\n",
    "ratings_df = pd.DataFrame(ratings_data)\n",
    "\n",
    "print(\"Synthetic Dataset with Non-linear Patterns:\")\n",
    "print(f\"Number of users: {n_users}\")\n",
    "print(f\"Number of items: {n_items}\")\n",
    "print(f\"Number of ratings: {len(ratings_df)}\")\n",
    "\n",
    "# Prepare data\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = RatingDataset(train_df)\n",
    "val_dataset = RatingDataset(val_df)\n",
    "test_dataset = RatingDataset(test_df)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Train different models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "models = {\n",
    "    'NCF': NCF(train_dataset.n_users, train_dataset.n_items, n_factors=10, layers=[20, 10]),\n",
    "    'Wide&Deep': WideAndDeep(train_dataset.n_users, train_dataset.n_items, n_factors=10, deep_layers=[20, 10]),\n",
    "    'NeuMF': NeuMF(train_dataset.n_users, train_dataset.n_items, n_factors=10, mlp_layers=[20, 10])\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader, n_epochs=50, device=device)\n",
    "    \n",
    "    # Evaluate\n",
    "    mae, rmse, predictions, actuals = evaluate_model(model, test_loader, device=device)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'predictions': predictions,\n",
    "        'actuals': actuals\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Plot 1: Training curves\n",
    "plt.subplot(3, 4, 1)\n",
    "for name, result in results.items():\n",
    "    plt.plot(result['train_losses'], label=f'{name} Train')\n",
    "    plt.plot(result['val_losses'], label=f'{name} Val', linestyle='--')\n",
    "plt.title('Training Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Model comparison - MAE\n",
    "plt.subplot(3, 4, 2)\n",
    "mae_values = [results[name]['mae'] for name in results.keys()]\n",
    "plt.bar(results.keys(), mae_values, color=['blue', 'red', 'green'])\n",
    "plt.title('MAE Comparison')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "\n",
    "# Plot 3: Model comparison - RMSE\n",
    "plt.subplot(3, 4, 3)\n",
    "rmse_values = [results[name]['rmse'] for name in results.keys()]\n",
    "plt.bar(results.keys(), rmse_values, color=['blue', 'red', 'green'])\n",
    "plt.title('RMSE Comparison')\n",
    "plt.ylabel('Root Mean Square Error')\n",
    "\n",
    "# Plot 4-6: Prediction vs Actual for each model\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    plt.subplot(3, 4, 4 + i)\n",
    "    plt.scatter(result['actuals'], result['predictions'], alpha=0.6)\n",
    "    plt.plot([1, 5], [1, 5], 'r--', alpha=0.8)\n",
    "    plt.title(f'{name}: Predicted vs Actual')\n",
    "    plt.xlabel('Actual Rating')\n",
    "    plt.ylabel('Predicted Rating')\n",
    "\n",
    "# Plot 7: Rating distribution\n",
    "plt.subplot(3, 4, 7)\n",
    "ratings_df['rating'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Rating Distribution')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Plot 8: Model architecture comparison\n",
    "plt.subplot(3, 4, 8)\n",
    "architectures = ['NCF', 'Wide&Deep', 'NeuMF']\n",
    "parameters = [\n",
    "    sum(p.numel() for p in models[name].parameters()) \n",
    "    for name in architectures\n",
    "]\n",
    "plt.bar(architectures, parameters)\n",
    "plt.title('Model Parameters')\n",
    "plt.ylabel('Number of Parameters')\n",
    "\n",
    "# Plot 9: Training time comparison (simulated)\n",
    "plt.subplot(3, 4, 9)\n",
    "training_times = [50, 45, 55]  # Simulated times\n",
    "plt.bar(architectures, training_times)\n",
    "plt.title('Training Time (epochs)')\n",
    "plt.ylabel('Time')\n",
    "\n",
    "# Plot 10: Convergence comparison\n",
    "plt.subplot(3, 4, 10)\n",
    "for name, result in results.items():\n",
    "    plt.plot(result['val_losses'], label=name, marker='o', markersize=3)\n",
    "plt.title('Convergence Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 11: Error distribution\n",
    "plt.subplot(3, 4, 11)\n",
    "for name, result in results.items():\n",
    "    errors = np.array(result['predictions']) - np.array(result['actuals'])\n",
    "    plt.hist(errors, bins=20, alpha=0.7, label=name)\n",
    "plt.title('Error Distribution')\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 12: Model summary\n",
    "plt.subplot(3, 4, 12)\n",
    "summary_data = {\n",
    "    'Model': list(results.keys()),\n",
    "    'MAE': [results[name]['mae'] for name in results.keys()],\n",
    "    'RMSE': [results[name]['rmse'] for name in results.keys()],\n",
    "    'Parameters': parameters\n",
    "}\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "plt.table(cellText=summary_df.values, colLabels=summary_df.columns, cellLoc='center', loc='center')\n",
    "plt.axis('off')\n",
    "plt.title('Model Summary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed analysis\n",
    "print(\"\\n=== Detailed Analysis ===\")\n",
    "\n",
    "# Compare model performance\n",
    "print(\"Model Performance Comparison:\")\n",
    "for name, result in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MAE: {result['mae']:.4f}\")\n",
    "    print(f\"  RMSE: {result['rmse']:.4f}\")\n",
    "    print(f\"  Parameters: {sum(p.numel() for p in result['model'].parameters()):,}\")\n",
    "    print()\n",
    "\n",
    "# Analyze prediction patterns\n",
    "print(\"Prediction Pattern Analysis:\")\n",
    "for name, result in results.items():\n",
    "    predictions = np.array(result['predictions'])\n",
    "    actuals = np.array(result['actuals'])\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Prediction range: [{predictions.min():.3f}, {predictions.max():.3f}]\")\n",
    "    print(f\"  Prediction std: {predictions.std():.3f}\")\n",
    "    print(f\"  Bias: {predictions.mean() - actuals.mean():.3f}\")\n",
    "    print()\n",
    "\n",
    "# Test recommendations\n",
    "print(\"Recommendation Test:\")\n",
    "test_user = 0\n",
    "test_item = 0\n",
    "\n",
    "# Find user and item in test set\n",
    "user_mapping = {user: idx for idx, user in enumerate(ratings_df['user_id'].unique())}\n",
    "item_mapping = {item: idx for idx, item in enumerate(ratings_df['item_id'].unique())}\n",
    "\n",
    "if test_user in user_mapping and test_item in item_mapping:\n",
    "    user_idx = torch.tensor([user_mapping[test_user]]).to(device)\n",
    "    item_idx = torch.tensor([item_mapping[test_item]]).to(device)\n",
    "    \n",
    "    print(f\"Predictions for User {test_user}, Item {test_item}:\")\n",
    "    for name, result in results.items():\n",
    "        model = result['model'].to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(user_idx, item_idx).cpu().numpy()[0]\n",
    "        print(f\"  {name}: {pred:.3f}\")\n",
    "```\n",
    "\n",
    "### R Implementation\n",
    "\n",
    "``` r\n",
    "# Deep Recommender Systems in R\n",
    "library(keras)\n",
    "library(tensorflow)\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "library(tidyr)\n",
    "\n",
    "# Generate synthetic data\n",
    "set.seed(42)\n",
    "n_users <- 500\n",
    "n_items <- 300\n",
    "n_ratings <- 3000\n",
    "\n",
    "# Create synthetic ratings with non-linear patterns\n",
    "ratings_data <- list()\n",
    "for (user_id in 1:n_users) {\n",
    "  n_user_ratings <- sample(5:20, 1)\n",
    "  rated_items <- sample(1:n_items, n_user_ratings, replace = FALSE)\n",
    "  \n",
    "  for (item_id in rated_items) {\n",
    "    # Create non-linear patterns\n",
    "    user_factor <- rnorm(1, 0, 1)\n",
    "    item_factor <- rnorm(1, 0, 1)\n",
    "    \n",
    "    # Non-linear interaction\n",
    "    interaction <- sin(user_factor) * cos(item_factor) + user_factor * item_factor\n",
    "    \n",
    "    # Add noise and convert to rating\n",
    "    rating <- max(1, min(5, 3 + interaction + rnorm(1, 0, 0.3)))\n",
    "    \n",
    "    ratings_data[[length(ratings_data) + 1]] <- list(\n",
    "      user_id = user_id,\n",
    "      item_id = item_id,\n",
    "      rating = rating\n",
    "    )\n",
    "  }\n",
    "}\n",
    "\n",
    "ratings_df <- do.call(rbind, lapply(ratings_data, as.data.frame))\n",
    "\n",
    "# Create user and item mappings\n",
    "user_mapping <- setNames(1:length(unique(ratings_df$user_id)), unique(ratings_df$user_id))\n",
    "item_mapping <- setNames(1:length(unique(ratings_df$item_id)), unique(ratings_df$item_id))\n",
    "\n",
    "# Convert to indices\n",
    "ratings_df$user_idx <- user_mapping[as.character(ratings_df$user_id)]\n",
    "ratings_df$item_idx <- item_mapping[as.character(ratings_df$item_id)]\n",
    "\n",
    "# Split data\n",
    "set.seed(42)\n",
    "train_indices <- sample(1:nrow(ratings_df), 0.8 * nrow(ratings_df))\n",
    "train_df <- ratings_df[train_indices, ]\n",
    "test_df <- ratings_df[-train_indices, ]\n",
    "\n",
    "# Prepare data for Keras\n",
    "n_users <- length(unique(ratings_df$user_id))\n",
    "n_items <- length(unique(ratings_df$item_id))\n",
    "n_factors <- 10\n",
    "\n",
    "# NCF Model\n",
    "build_ncf_model <- function() {\n",
    "  # Input layers\n",
    "  user_input <- layer_input(shape = 1, name = \"user_input\")\n",
    "  item_input <- layer_input(shape = 1, name = \"item_input\")\n",
    "  \n",
    "  # Embeddings\n",
    "  user_embedding <- user_input %>%\n",
    "    layer_embedding(input_dim = n_users, output_dim = n_factors, name = \"user_embedding\") %>%\n",
    "    layer_flatten()\n",
    "  \n",
    "  item_embedding <- item_input %>%\n",
    "    layer_embedding(input_dim = n_items, output_dim = n_factors, name = \"item_embedding\") %>%\n",
    "    layer_flatten()\n",
    "  \n",
    "  # Concatenate\n",
    "  concat <- layer_concatenate(list(user_embedding, item_embedding))\n",
    "  \n",
    "  # MLP layers\n",
    "  mlp <- concat %>%\n",
    "    layer_dense(units = 20, activation = \"relu\") %>%\n",
    "    layer_dropout(rate = 0.1) %>%\n",
    "    layer_dense(units = 10, activation = \"relu\") %>%\n",
    "    layer_dropout(rate = 0.1) %>%\n",
    "    layer_dense(units = 1, activation = \"linear\")\n",
    "  \n",
    "  # Create model\n",
    "  model <- keras_model(inputs = list(user_input, item_input), outputs = mlp)\n",
    "  \n",
    "  # Compile\n",
    "  model %>% compile(\n",
    "    optimizer = optimizer_adam(learning_rate = 0.001),\n",
    "    loss = \"mse\",\n",
    "    metrics = c(\"mae\")\n",
    "  )\n",
    "  \n",
    "  return(model)\n",
    "}\n",
    "\n",
    "# Train NCF model\n",
    "ncf_model <- build_ncf_model()\n",
    "\n",
    "# Prepare training data\n",
    "user_indices <- train_df$user_idx - 1  # Keras uses 0-based indexing\n",
    "item_indices <- train_df$item_idx - 1\n",
    "ratings <- train_df$rating\n",
    "\n",
    "# Train model\n",
    "history <- ncf_model %>% fit(\n",
    "  list(user_indices, item_indices),\n",
    "  ratings,\n",
    "  epochs = 50,\n",
    "  batch_size = 64,\n",
    "  validation_split = 0.2,\n",
    "  verbose = 1\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "test_user_indices <- test_df$user_idx - 1\n",
    "test_item_indices <- test_df$item_idx - 1\n",
    "test_ratings <- test_df$rating\n",
    "\n",
    "predictions <- ncf_model %>% predict(list(test_user_indices, test_item_indices))\n",
    "mae <- mean(abs(predictions - test_ratings))\n",
    "rmse <- sqrt(mean((predictions - test_ratings)^2))\n",
    "\n",
    "cat(\"NCF Model Results:\\n\")\n",
    "cat(\"MAE:\", mae, \"\\n\")\n",
    "cat(\"RMSE:\", rmse, \"\\n\")\n",
    "\n",
    "# Visualization\n",
    "# Training history\n",
    "p1 <- ggplot(data.frame(\n",
    "  epoch = 1:length(history$metrics$loss),\n",
    "  loss = history$metrics$loss,\n",
    "  val_loss = history$metrics$val_loss\n",
    ")) +\n",
    "  geom_line(aes(x = epoch, y = loss, color = \"Training\")) +\n",
    "  geom_line(aes(x = epoch, y = val_loss, color = \"Validation\")) +\n",
    "  labs(title = \"NCF Training History\",\n",
    "       x = \"Epoch\", y = \"Loss\", color = \"Dataset\") +\n",
    "  theme_minimal()\n",
    "\n",
    "# Prediction vs Actual\n",
    "p2 <- ggplot(data.frame(\n",
    "  actual = test_ratings,\n",
    "  predicted = predictions\n",
    ")) +\n",
    "  geom_point(aes(x = actual, y = predicted), alpha = 0.6) +\n",
    "  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n",
    "  labs(title = \"NCF: Predicted vs Actual\",\n",
    "       x = \"Actual Rating\", y = \"Predicted Rating\") +\n",
    "  theme_minimal()\n",
    "\n",
    "# Rating distribution\n",
    "p3 <- ggplot(ratings_df, aes(x = factor(rating))) +\n",
    "  geom_bar(fill = \"steelblue\") +\n",
    "  labs(title = \"Rating Distribution\",\n",
    "       x = \"Rating\", y = \"Count\") +\n",
    "  theme_minimal()\n",
    "\n",
    "# Combine plots\n",
    "library(gridExtra)\n",
    "grid.arrange(p1, p2, p3, ncol = 2)\n",
    "```\n",
    "\n",
    "## 13.7.6. Advanced Deep Learning Approaches\n",
    "\n",
    "### Attention Mechanisms in Recommender Systems\n",
    "\n",
    "#### Motivation and Intuition\n",
    "\n",
    "Attention mechanisms have revolutionized recommender systems by enabling\n",
    "models to focus on the most relevant parts of the input data. In\n",
    "recommendation contexts, attention helps models understand: - Which\n",
    "historical interactions are most relevant for predicting future\n",
    "preferences - How different features contribute to the final\n",
    "prediction - Temporal dynamics in sequential recommendation\n",
    "\n",
    "#### Mathematical Foundation\n",
    "\n",
    "**Attention as Weighted Sum**:\n",
    "$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n",
    "\n",
    "where: - $\\mathbf{Q} \\in \\mathbb{R}^{n_q \\times d_k}$: Query matrix -\n",
    "$\\mathbf{K} \\in \\mathbb{R}^{n_k \\times d_k}$: Key matrix -\n",
    "$\\mathbf{V} \\in \\mathbb{R}^{n_v \\times d_v}$: Value matrix - $d_k$:\n",
    "Dimension of keys and queries - $d_v$: Dimension of values\n",
    "\n",
    "#### Self-Attention for Sequential Recommendations\n",
    "\n",
    "**Problem Formulation**: Given a sequence of user interactions\n",
    "$[r_1, r_2, \\ldots, r_t]$, predict the next interaction $r_{t+1}$.\n",
    "\n",
    "**Attention Computation**:\n",
    "$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n",
    "\n",
    "**Mathematical Components**:\n",
    "\n",
    "1.  **Query, Key, Value Generation**:\n",
    "    $$\\mathbf{Q} = \\mathbf{H}\\mathbf{W}_Q, \\quad \\mathbf{K} = \\mathbf{H}\\mathbf{W}_K, \\quad \\mathbf{V} = \\mathbf{H}\\mathbf{W}_V$$\n",
    "    where $\\mathbf{H}$ is the sequence of hidden states.\n",
    "\n",
    "2.  **Attention Weights**:\n",
    "    $$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^t \\exp(e_{ik})}$$\n",
    "    where $e_{ij} = \\frac{\\mathbf{q}_i^T \\mathbf{k}_j}{\\sqrt{d_k}}$.\n",
    "\n",
    "3.  **Output Computation**:\n",
    "    $$\\mathbf{o}_i = \\sum_{j=1}^t \\alpha_{ij} \\mathbf{v}_j$$\n",
    "\n",
    "#### Multi-Head Attention\n",
    "\n",
    "Multi-head attention allows the model to attend to different aspects of\n",
    "the input simultaneously:\n",
    "\n",
    "$$\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)\\mathbf{W}^O$$\n",
    "\n",
    "where each head is computed as:\n",
    "$$\\text{head}_i = \\text{Attention}(\\mathbf{Q}\\mathbf{W}_i^Q, \\mathbf{K}\\mathbf{W}_i^K, \\mathbf{V}\\mathbf{W}_i^V)$$\n",
    "\n",
    "**Mathematical Properties**: - **Parallel Processing**: Multiple\n",
    "attention heads can be computed in parallel - **Diverse\n",
    "Representations**: Each head can focus on different aspects -\n",
    "**Scalability**: Can be efficiently implemented on modern hardware\n",
    "\n",
    "#### Positional Encoding\n",
    "\n",
    "Since attention is permutation-invariant, positional information must be\n",
    "added:\n",
    "\n",
    "$$\\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "\\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "### Graph Neural Networks for Recommendations\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "Graph Neural Networks (GNNs) are particularly well-suited for\n",
    "recommender systems because: - User-item interactions naturally form a\n",
    "bipartite graph - GNNs can capture high-order connectivity patterns -\n",
    "They can incorporate both user-user and item-item similarities\n",
    "\n",
    "#### Mathematical Foundation\n",
    "\n",
    "**Graph Representation**:\n",
    "$$\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$$\n",
    "\n",
    "where: - $\\mathcal{V} = \\mathcal{U} \\cup \\mathcal{I}$: Set of users and\n",
    "items - $\\mathcal{E}$: Set of edges representing interactions\n",
    "\n",
    "**Adjacency Matrix**:\n",
    "$$\\mathbf{A}_{ij} = \\begin{cases}\n",
    "1 & \\text{if } (i,j) \\in \\mathcal{E} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "#### Graph Convolutional Networks (GCN)\n",
    "\n",
    "**Message Passing Framework**:\n",
    "$$\\mathbf{h}_i^{(l+1)} = \\sigma\\left(\\mathbf{W}^{(l)} \\sum_{j \\in \\mathcal{N}(i)} \\frac{1}{\\sqrt{|\\mathcal{N}(i)||\\mathcal{N}(j)|}} \\mathbf{h}_j^{(l)}\\right)$$\n",
    "\n",
    "**Matrix Form**:\n",
    "$$\\mathbf{H}^{(l+1)} = \\sigma\\left(\\tilde{\\mathbf{D}}^{-\\frac{1}{2}}\\tilde{\\mathbf{A}}\\tilde{\\mathbf{D}}^{-\\frac{1}{2}}\\mathbf{H}^{(l)}\\mathbf{W}^{(l)}\\right)$$\n",
    "\n",
    "where: - $\\tilde{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$: Adjacency\n",
    "matrix with self-loops - $\\tilde{\\mathbf{D}}$: Degree matrix of\n",
    "$\\tilde{\\mathbf{A}}$ - $\\mathbf{H}^{(l)}$: Node features at layer $l$\n",
    "\n",
    "**Mathematical Properties**: - **Normalization**: Prevents numerical\n",
    "instability - **Self-loops**: Allows nodes to retain their own\n",
    "information - **Symmetric**: Ensures stable training\n",
    "\n",
    "#### Graph Attention Networks (GAT)\n",
    "\n",
    "GAT introduces learnable attention weights for neighbor aggregation:\n",
    "\n",
    "**Attention Mechanism**:\n",
    "$$\\alpha_{ij} = \\frac{\\exp(\\text{LeakyReLU}(\\mathbf{a}^T[\\mathbf{W}\\mathbf{h}_i \\| \\mathbf{W}\\mathbf{h}_j]))}{\\sum_{k \\in \\mathcal{N}_i} \\exp(\\text{LeakyReLU}(\\mathbf{a}^T[\\mathbf{W}\\mathbf{h}_i \\| \\mathbf{W}\\mathbf{h}_k]))}$$\n",
    "\n",
    "**Node Update**:\n",
    "$$\\mathbf{h}_i^{(l+1)} = \\sigma\\left(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^{(l)} \\mathbf{W}^{(l)} \\mathbf{h}_j^{(l)}\\right)$$\n",
    "\n",
    "**Multi-head Attention**:\n",
    "$$\\mathbf{h}_i^{(l+1)} = \\sigma\\left(\\frac{1}{K} \\sum_{k=1}^K \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^{(l,k)} \\mathbf{W}^{(l,k)} \\mathbf{h}_j^{(l)}\\right)$$\n",
    "\n",
    "#### GraphSAGE\n",
    "\n",
    "GraphSAGE uses a different aggregation strategy:\n",
    "\n",
    "$$\\mathbf{h}_{\\mathcal{N}(i)}^{(l)} = \\text{AGGREGATE}^{(l)}\\left(\\{\\mathbf{h}_j^{(l-1)}, \\forall j \\in \\mathcal{N}(i)\\}\\right)\n",
    "\\mathbf{h}_i^{(l)} = \\sigma\\left(\\mathbf{W}^{(l)} \\cdot [\\mathbf{h}_i^{(l-1)} \\| \\mathbf{h}_{\\mathcal{N}(i)}^{(l)}]\\right)$$\n",
    "\n",
    "**Aggregation Functions**: - **Mean**:\n",
    "$\\text{AGGREGATE} = \\frac{1}{|\\mathcal{N}(i)|} \\sum_{j \\in \\mathcal{N}(i)} \\mathbf{h}_j$ -\n",
    "**Max**: $\\text{AGGREGATE} = \\max_{j \\in \\mathcal{N}(i)} \\mathbf{h}_j$ -\n",
    "**LSTM**:\n",
    "$\\text{AGGREGATE} = \\text{LSTM}(\\{\\mathbf{h}_j\\}_{j \\in \\mathcal{N}(i)})$\n",
    "\n",
    "### Transformer-based Models\n",
    "\n",
    "#### BERT4Rec Architecture\n",
    "\n",
    "BERT4Rec adapts the BERT architecture for sequential recommendation:\n",
    "\n",
    "**Input Representation**:\n",
    "$$\\mathbf{x}_t = \\text{Embedding}(r_t) + \\text{PositionalEncoding}(t)$$\n",
    "\n",
    "**Multi-Head Self-Attention**:\n",
    "$$\\text{MultiHead}(\\mathbf{X}) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)\\mathbf{W}^O$$\n",
    "\n",
    "**Feed-Forward Network**:\n",
    "$$\\text{FFN}(\\mathbf{x}) = \\mathbf{W}_2 \\text{ReLU}(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2$$\n",
    "\n",
    "**Layer Normalization**:\n",
    "$$\\text{LayerNorm}(\\mathbf{x}) = \\gamma \\frac{\\mathbf{x} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "**Prediction**:\n",
    "$$P(r_t | r_1, \\ldots, r_{t-1}) = \\text{softmax}(\\mathbf{W}\\mathbf{h}_t + \\mathbf{b})$$\n",
    "\n",
    "#### Training Strategy\n",
    "\n",
    "**Masked Language Modeling**:\n",
    "$$\\mathcal{L} = -\\sum_{t \\in \\mathcal{M}} \\log P(r_t | r_1, \\ldots, r_{t-1})$$\n",
    "\n",
    "where $\\mathcal{M}$ is the set of masked positions.\n",
    "\n",
    "**Next Sentence Prediction** (adapted):\n",
    "$$\\mathcal{L}_{\\text{NSP}} = -\\log P(\\text{IsNext} | \\text{sequence}_1, \\text{sequence}_2)$$\n",
    "\n",
    "### Advanced Attention Variants\n",
    "\n",
    "#### Relative Positional Encoding\n",
    "\n",
    "Instead of absolute positions, use relative positions:\n",
    "\n",
    "$$e_{ij} = \\frac{\\mathbf{q}_i^T \\mathbf{k}_j}{\\sqrt{d_k}} + \\mathbf{q}_i^T \\mathbf{r}_{i-j}$$\n",
    "\n",
    "where $\\mathbf{r}_{i-j}$ is the relative position embedding.\n",
    "\n",
    "#### Sparse Attention\n",
    "\n",
    "For efficiency, use sparse attention patterns:\n",
    "\n",
    "$$\\text{SparseAttention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} \\odot \\mathbf{M}\\right)\\mathbf{V}$$\n",
    "\n",
    "where $\\mathbf{M}$ is a sparse mask.\n",
    "\n",
    "#### Local Attention\n",
    "\n",
    "Restrict attention to a local window:\n",
    "\n",
    "$$\\alpha_{ij} = \\begin{cases}\n",
    "\\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{W}_i} \\exp(e_{ik})} & \\text{if } j \\in \\mathcal{W}_i \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "where $\\mathcal{W}_i$ is the local window around position $i$.\n",
    "\n",
    "### Theoretical Analysis\n",
    "\n",
    "#### 1. Expressiveness\n",
    "\n",
    "**Theorem**: Attention mechanisms can approximate any continuous\n",
    "function on sequences.\n",
    "\n",
    "**Proof Sketch**: Attention can be viewed as a universal approximator\n",
    "for sequence-to-sequence functions.\n",
    "\n",
    "#### 2. Computational Complexity\n",
    "\n",
    "**Time Complexity**: $O(n^2 d_k)$ for self-attention **Space\n",
    "Complexity**: $O(n^2)$ for storing attention weights\n",
    "\n",
    "#### 3. Convergence Properties\n",
    "\n",
    "**Theorem**: Under appropriate conditions, attention-based models\n",
    "converge to local minima.\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "#### 1. Hyperparameter Tuning\n",
    "\n",
    "**Attention**: - Number of heads: $h = 4-16$ - Attention dimension:\n",
    "$d_k = 64-512$ - Dropout rate: $p = 0.1-0.3$\n",
    "\n",
    "**GNN**: - Number of layers: $L = 2-4$ - Hidden dimension:\n",
    "$d = 64-256$ - Aggregation function: Mean/Max/LSTM\n",
    "\n",
    "#### 2. Training Strategies\n",
    "\n",
    "**Curriculum Learning**: 1. Start with short sequences 2. Gradually\n",
    "increase sequence length 3. Add complexity progressively\n",
    "\n",
    "**Regularization**: - Dropout on attention weights - L2 regularization\n",
    "on parameters - Early stopping based on validation\n",
    "\n",
    "#### 3. Scalability\n",
    "\n",
    "**Efficient Attention**: - Sparse attention patterns - Linear attention\n",
    "approximations - Hierarchical attention structures\n",
    "\n",
    "**Graph Sampling**: - Node sampling for large graphs - Edge sampling for\n",
    "sparse graphs - Subgraph sampling for training\n",
    "\n",
    "This comprehensive mathematical foundation provides the theoretical\n",
    "understanding and practical guidance needed to implement advanced deep\n",
    "learning approaches in recommender systems.\n",
    "\n",
    "## 13.7.7. Multi-modal Deep Learning\n",
    "\n",
    "### Motivation and Problem Statement\n",
    "\n",
    "Multi-modal deep learning in recommender systems addresses the challenge\n",
    "of integrating diverse data types to improve recommendation quality.\n",
    "Modern recommendation scenarios often involve multiple modalities: -\n",
    "**Text**: Item descriptions, user reviews, product titles - **Images**:\n",
    "Product photos, user profile pictures, visual content - **Audio**:\n",
    "Music, podcasts, voice content - **Video**: Movies, tutorials, live\n",
    "streams - **Structured Data**: User demographics, item categories,\n",
    "ratings\n",
    "\n",
    "#### Why Multi-modal Learning?\n",
    "\n",
    "1.  **Complementary Information**: Different modalities provide\n",
    "    complementary information about users and items\n",
    "2.  **Cold Start Mitigation**: Visual and textual features help with new\n",
    "    user/item recommendations\n",
    "3.  **Rich Representations**: Multi-modal data enables richer\n",
    "    understanding of user preferences\n",
    "4.  **Cross-modal Discovery**: Can discover relationships between\n",
    "    different modalities\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "#### Multi-modal Data Representation\n",
    "\n",
    "**Input Space**:\n",
    "$\\mathcal{X} = \\mathcal{X}_1 \\times \\mathcal{X}_2 \\times \\cdots \\times \\mathcal{X}_M$\n",
    "\n",
    "where $\\mathcal{X}_i$ represents the space of modality $i$.\n",
    "\n",
    "**Feature Extraction**:\n",
    "$$\\mathbf{f}_i = \\text{Encoder}_i(\\mathbf{x}_i) \\in \\mathbb{R}^{d_i}$$\n",
    "\n",
    "where $\\text{Encoder}_i$ is a neural network for modality $i$.\n",
    "\n",
    "#### Fusion Strategies\n",
    "\n",
    "**1. Early Fusion (Feature-level)**:\n",
    "$$\\mathbf{f}_{\\text{fused}} = \\text{Fusion}([\\mathbf{f}_1, \\mathbf{f}_2, \\ldots, \\mathbf{f}_M])$$\n",
    "\n",
    "**2. Late Fusion (Decision-level)**:\n",
    "$$\\hat{r}_{ui} = \\sum_{i=1}^M \\alpha_i \\cdot \\text{Predictor}_i(\\mathbf{f}_i)$$\n",
    "\n",
    "**3. Hybrid Fusion**:\n",
    "$$\\mathbf{f}_{\\text{fused}} = \\text{Fusion}(\\text{Encoder}_1(\\mathbf{x}_1), \\ldots, \\text{Encoder}_M(\\mathbf{x}_M))$$\n",
    "\n",
    "### Text + Image Recommendations\n",
    "\n",
    "#### Multi-modal Fusion\n",
    "\n",
    "**Weighted Sum Fusion**:\n",
    "$$\\mathbf{f}_{\\text{fused}} = \\alpha \\cdot \\mathbf{f}_{\\text{text}} + (1-\\alpha) \\cdot \\mathbf{f}_{\\text{image}}$$\n",
    "\n",
    "where $\\alpha$ is learned during training.\n",
    "\n",
    "**Concatenation Fusion**:\n",
    "$$\\mathbf{f}_{\\text{fused}} = [\\mathbf{f}_{\\text{text}}; \\mathbf{f}_{\\text{image}}]$$\n",
    "\n",
    "**Bilinear Fusion**:\n",
    "$$\\mathbf{f}_{\\text{fused}} = \\mathbf{f}_{\\text{text}}^T \\mathbf{W} \\mathbf{f}_{\\text{image}}$$\n",
    "\n",
    "where $\\mathbf{W}$ is a learnable bilinear transformation matrix.\n",
    "\n",
    "#### Cross-modal Attention\n",
    "\n",
    "**Attention Mechanism**:\n",
    "$$\\text{Attention}_{\\text{cross}}(\\mathbf{q}, \\mathbf{K}) = \\text{softmax}\\left(\\frac{\\mathbf{q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "**Cross-modal Attention**:\n",
    "$$\\alpha_{ij} = \\frac{\\exp(\\mathbf{q}_i^T \\mathbf{k}_j / \\sqrt{d_k})}{\\sum_{l=1}^N \\exp(\\mathbf{q}_i^T \\mathbf{k}_l / \\sqrt{d_k})}$$\n",
    "\n",
    "where $\\mathbf{q}_i$ is a query from one modality and $\\mathbf{k}_j$ is\n",
    "a key from another modality.\n",
    "\n",
    "#### Text Encoding\n",
    "\n",
    "**BERT for Text**:\n",
    "$$\\mathbf{f}_{\\text{text}} = \\text{BERT}(\\text{tokenize}(\\text{description}))$$\n",
    "\n",
    "**Word Embeddings**:\n",
    "$$\\mathbf{f}_{\\text{text}} = \\frac{1}{L} \\sum_{i=1}^L \\mathbf{e}_i$$\n",
    "\n",
    "where $\\mathbf{e}_i$ is the embedding of word $i$ and $L$ is the\n",
    "sequence length.\n",
    "\n",
    "#### Image Encoding\n",
    "\n",
    "**CNN for Images**:\n",
    "$$\\mathbf{f}_{\\text{image}} = \\text{CNN}(\\text{image})$$\n",
    "\n",
    "**Pre-trained Models**:\n",
    "$$\\mathbf{f}_{\\text{image}} = \\text{ResNet}(\\text{image}) \\text{ or } \\text{ViT}(\\text{image})$$\n",
    "\n",
    "### Audio + Visual Recommendations\n",
    "\n",
    "#### Temporal Fusion\n",
    "\n",
    "**LSTM-based Fusion**:\n",
    "$$\\mathbf{h}_t = \\text{LSTM}([\\mathbf{a}_t; \\mathbf{v}_t], \\mathbf{h}_{t-1})$$\n",
    "\n",
    "where $\\mathbf{a}_t$ and $\\mathbf{v}_t$ are audio and visual features at\n",
    "time $t$.\n",
    "\n",
    "**Attention-based Fusion**:\n",
    "$$\\alpha_t = \\text{softmax}(\\mathbf{W}_a \\mathbf{a}_t + \\mathbf{W}_v \\mathbf{v}_t)\n",
    "\\mathbf{h}_t = \\alpha_t \\cdot \\mathbf{a}_t + (1-\\alpha_t) \\cdot \\mathbf{v}_t$$\n",
    "\n",
    "#### Audio Feature Extraction\n",
    "\n",
    "**Mel-frequency Cepstral Coefficients (MFCC)**:\n",
    "$$\\mathbf{f}_{\\text{audio}} = \\text{MFCC}(\\text{audio\\_signal})$$\n",
    "\n",
    "**Spectrogram Features**:\n",
    "$$\\mathbf{f}_{\\text{audio}} = \\text{CNN}(\\text{spectrogram})$$\n",
    "\n",
    "#### Video Feature Extraction\n",
    "\n",
    "**3D CNN**:\n",
    "$$\\mathbf{f}_{\\text{video}} = \\text{3D-CNN}(\\text{video\\_frames})$$\n",
    "\n",
    "**Two-stream Architecture**:\n",
    "$$\\mathbf{f}_{\\text{video}} = \\text{Fusion}(\\text{Spatial-CNN}(\\text{frames}), \\text{Temporal-CNN}(\\text{optical\\_flow}))$$\n",
    "\n",
    "### Advanced Multi-modal Architectures\n",
    "\n",
    "#### 1. Cross-modal Transformer\n",
    "\n",
    "**Cross-modal Attention**:\n",
    "$$\\text{CrossAttention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n",
    "\n",
    "where $\\mathbf{Q}$ comes from one modality and $\\mathbf{K}, \\mathbf{V}$\n",
    "come from another.\n",
    "\n",
    "**Multi-modal Transformer**:\n",
    "$$\\mathbf{h}_{\\text{mm}} = \\text{Transformer}(\\text{Concat}(\\mathbf{f}_{\\text{text}}, \\mathbf{f}_{\\text{image}}))$$\n",
    "\n",
    "#### 2. Multi-modal Variational Autoencoder (MMVAE)\n",
    "\n",
    "**Encoder**:\n",
    "$$q(\\mathbf{z} | \\mathbf{x}_1, \\mathbf{x}_2) = \\mathcal{N}(\\mu_{\\text{mm}}, \\sigma_{\\text{mm}}^2)$$\n",
    "\n",
    "**Decoder**:\n",
    "$$p(\\mathbf{x}_i | \\mathbf{z}) = \\text{Decoder}_i(\\mathbf{z})$$\n",
    "\n",
    "**Loss Function**:\n",
    "$$\\mathcal{L} = \\mathbb{E}_{q(\\mathbf{z})}[\\log p(\\mathbf{x}_1, \\mathbf{x}_2 | \\mathbf{z})] - \\text{KL}(q(\\mathbf{z}) \\| p(\\mathbf{z}))$$\n",
    "\n",
    "#### 3. Contrastive Learning\n",
    "\n",
    "**Multi-modal Contrastive Loss**:\n",
    "$$\\mathcal{L}_{\\text{contrastive}} = -\\log \\frac{\\exp(\\text{sim}(\\mathbf{f}_1, \\mathbf{f}_2) / \\tau)}{\\sum_{i=1}^N \\exp(\\text{sim}(\\mathbf{f}_1, \\mathbf{f}_2^{(i)}) / \\tau)}$$\n",
    "\n",
    "where $\\text{sim}(\\cdot, \\cdot)$ is a similarity function and $\\tau$ is\n",
    "the temperature parameter.\n",
    "\n",
    "### Mathematical Analysis\n",
    "\n",
    "#### 1. Modality Alignment\n",
    "\n",
    "**Alignment Loss**:\n",
    "$$\\mathcal{L}_{\\text{align}} = \\|\\mathbf{f}_1 - \\mathbf{f}_2\\|_2^2$$\n",
    "\n",
    "**Canonical Correlation Analysis (CCA)**:\n",
    "$$\\max_{\\mathbf{w}_1, \\mathbf{w}_2} \\frac{\\mathbf{w}_1^T \\mathbf{C}_{12} \\mathbf{w}_2}{\\sqrt{\\mathbf{w}_1^T \\mathbf{C}_{11} \\mathbf{w}_1 \\mathbf{w}_2^T \\mathbf{C}_{22} \\mathbf{w}_2}}$$\n",
    "\n",
    "where $\\mathbf{C}_{ij}$ is the cross-covariance matrix between\n",
    "modalities $i$ and $j$.\n",
    "\n",
    "#### 2. Modality-specific Losses\n",
    "\n",
    "**Reconstruction Loss**:\n",
    "$$\\mathcal{L}_{\\text{recon}} = \\sum_{i=1}^M \\|\\mathbf{x}_i - \\text{Decoder}_i(\\mathbf{f}_{\\text{fused}})\\|_2^2$$\n",
    "\n",
    "**Classification Loss**:\n",
    "$$\\mathcal{L}_{\\text{class}} = -\\sum_{c=1}^C y_c \\log(\\hat{y}_c)$$\n",
    "\n",
    "#### 3. Multi-task Learning\n",
    "\n",
    "**Joint Loss**:\n",
    "$$\\mathcal{L}_{\\text{total}} = \\sum_{i=1}^M \\lambda_i \\mathcal{L}_i$$\n",
    "\n",
    "where $\\lambda_i$ are task-specific weights.\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "#### 1. Data Preprocessing\n",
    "\n",
    "**Text Processing**: - Tokenization and vocabulary building - Sequence\n",
    "padding and truncation - Pre-trained embeddings (Word2Vec, GloVe, BERT)\n",
    "\n",
    "**Image Processing**: - Resizing and normalization - Data augmentation\n",
    "(rotation, cropping, color jittering) - Pre-trained models (ResNet, ViT,\n",
    "CLIP)\n",
    "\n",
    "**Audio Processing**: - Sampling rate normalization - Spectrogram\n",
    "computation - MFCC extraction\n",
    "\n",
    "#### 2. Architecture Design\n",
    "\n",
    "**Modality-specific Encoders**: - Text: BERT, LSTM, Transformer - Image:\n",
    "CNN, ViT, ResNet - Audio: 1D-CNN, LSTM, Transformer\n",
    "\n",
    "**Fusion Strategies**: - Early fusion: Concatenation, weighted sum -\n",
    "Late fusion: Ensemble, voting - Attention fusion: Cross-modal attention\n",
    "\n",
    "#### 3. Training Strategies\n",
    "\n",
    "**Curriculum Learning**: 1. Train modality-specific encoders separately\n",
    "2. Train fusion module with frozen encoders 3. Fine-tune entire model\n",
    "end-to-end\n",
    "\n",
    "**Multi-task Learning**: - Modality-specific tasks (text classification,\n",
    "image classification) - Joint recommendation task - Auxiliary tasks\n",
    "(modality prediction, alignment)\n",
    "\n",
    "#### 4. Evaluation Metrics\n",
    "\n",
    "**Modality-specific Metrics**: - Text: BLEU, ROUGE, BERTScore - Image:\n",
    "PSNR, SSIM, FID - Audio: PESQ, STOI\n",
    "\n",
    "**Joint Metrics**: - Recommendation accuracy: Precision@k, Recall@k,\n",
    "NDCG@k - Modality alignment: CCA correlation, alignment loss -\n",
    "Cross-modal retrieval: R@k, mAP\n",
    "\n",
    "### Challenges and Solutions\n",
    "\n",
    "#### 1. Modality Imbalance\n",
    "\n",
    "**Problem**: Some modalities may dominate the learning process.\n",
    "\n",
    "**Solutions**: - Modality-specific learning rates - Balanced sampling\n",
    "strategies - Attention mechanisms\n",
    "\n",
    "#### 2. Missing Modalities\n",
    "\n",
    "**Problem**: Not all items have all modalities available.\n",
    "\n",
    "**Solutions**: - Zero-padding for missing modalities - Modality-specific\n",
    "encoders with dropout - Generative models to fill missing modalities\n",
    "\n",
    "#### 3. Computational Complexity\n",
    "\n",
    "**Problem**: Multi-modal models are computationally expensive.\n",
    "\n",
    "**Solutions**: - Modality-specific pre-training - Efficient fusion\n",
    "strategies - Model compression techniques\n",
    "\n",
    "This comprehensive mathematical foundation provides the theoretical\n",
    "understanding and practical guidance needed to implement multi-modal\n",
    "deep learning approaches in recommender systems.\n",
    "\n",
    "## 13.7.8. Evaluation and Optimization\n",
    "\n",
    "### Loss Functions for Deep Recommender Systems\n",
    "\n",
    "#### Motivation and Problem Formulation\n",
    "\n",
    "The choice of loss function is crucial for deep recommender systems as\n",
    "it directly influences what the model learns. Different recommendation\n",
    "scenarios require different loss functions:\n",
    "\n",
    "1.  **Rating Prediction**: Predict exact rating values\n",
    "2.  **Click Prediction**: Predict binary interaction (click/no-click)\n",
    "3.  **Ranking**: Predict relative preferences between items\n",
    "4.  **Multi-label**: Predict multiple relevant items\n",
    "\n",
    "#### Mathematical Foundation\n",
    "\n",
    "**General Loss Function**:\n",
    "$$\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\ell(f_\\theta(\\mathbf{x}_i), y_i) + \\lambda R(\\theta)$$\n",
    "\n",
    "where: - $f_\\theta$ is the model with parameters $\\theta$ - $\\ell$ is\n",
    "the loss function - $R(\\theta)$ is the regularization term - $\\lambda$\n",
    "is the regularization strength\n",
    "\n",
    "### Rating Prediction Losses\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "$$\\mathcal{L}_{\\text{MSE}} = \\frac{1}{N} \\sum_{(u,i) \\in \\mathcal{R}} (r_{ui} - \\hat{r}_{ui})^2$$\n",
    "\n",
    "**Properties**: - **Convex**: Guarantees convergence to global minimum -\n",
    "**Sensitive to Outliers**: Large errors are penalized heavily -\n",
    "**Scale-dependent**: Sensitive to the scale of ratings\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "$$\\mathcal{L}_{\\text{MAE}} = \\frac{1}{N} \\sum_{(u,i) \\in \\mathcal{R}} |r_{ui} - \\hat{r}_{ui}|$$\n",
    "\n",
    "**Properties**: - **Robust to Outliers**: Less sensitive to extreme\n",
    "values - **Non-differentiable**: Requires subgradient methods -\n",
    "**Scale-invariant**: Relative to the rating scale\n",
    "\n",
    "#### Huber Loss\n",
    "\n",
    "$$\\mathcal{L}_{\\text{Huber}} = \\frac{1}{N} \\sum_{(u,i) \\in \\mathcal{R}} \\begin{cases}\n",
    "\\frac{1}{2}(r_{ui} - \\hat{r}_{ui})^2 & \\text{if } |r_{ui} - \\hat{r}_{ui}| \\leq \\delta \\\\\n",
    "\\delta(|r_{ui} - \\hat{r}_{ui}| - \\frac{1}{2}\\delta) & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Properties**: - **Robust**: Combines benefits of MSE and MAE -\n",
    "**Differentiable**: Smooth everywhere - **Tunable**: $\\delta$ controls\n",
    "sensitivity to outliers\n",
    "\n",
    "### Binary Classification Losses\n",
    "\n",
    "#### Binary Cross-Entropy (BCE)\n",
    "\n",
    "$$\\mathcal{L}_{\\text{BCE}} = -\\frac{1}{N} \\sum_{(u,i) \\in \\mathcal{R}} [r_{ui} \\log(\\hat{r}_{ui}) + (1-r_{ui}) \\log(1-\\hat{r}_{ui})]$$\n",
    "\n",
    "**Properties**: - **Probabilistic**: Outputs can be interpreted as\n",
    "probabilities - **Well-calibrated**: Good for probability estimation -\n",
    "**Class-balanced**: Handles imbalanced data well\n",
    "\n",
    "#### Focal Loss\n",
    "\n",
    "$$\\mathcal{L}_{\\text{Focal}} = -\\frac{1}{N} \\sum_{(u,i) \\in \\mathcal{R}} \\alpha_t (1-\\hat{r}_{ui})^\\gamma \\log(\\hat{r}_{ui})$$\n",
    "\n",
    "where $\\alpha_t$ is the class weight and $\\gamma$ is the focusing\n",
    "parameter.\n",
    "\n",
    "**Properties**: - **Handles Imbalance**: Reduces impact of easy\n",
    "examples - **Adaptive**: Focuses on hard examples - **Tunable**:\n",
    "$\\gamma$ controls focusing strength\n",
    "\n",
    "### Ranking Losses\n",
    "\n",
    "#### Ranking Loss (Hinge Loss)\n",
    "\n",
    "$$\\mathcal{L}_{\\text{ranking}} = \\sum_{(u,i,j) \\in \\mathcal{D}} \\max(0, \\hat{r}_{uj} - \\hat{r}_{ui} + \\gamma)$$\n",
    "\n",
    "where $(u,i,j)$ represents user $u$ prefers item $i$ over item $j$.\n",
    "\n",
    "**Mathematical Properties**: - **Margin-based**: Enforces a margin\n",
    "$\\gamma$ between positive and negative pairs - **Pairwise**: Considers\n",
    "relative preferences - **Non-smooth**: Has non-differentiable points\n",
    "\n",
    "#### Bayesian Personalized Ranking (BPR)\n",
    "\n",
    "$$\\mathcal{L}_{\\text{BPR}} = -\\sum_{(u,i,j) \\in \\mathcal{D}} \\log(\\sigma(\\hat{r}_{ui} - \\hat{r}_{uj}))$$\n",
    "\n",
    "where $\\mathcal{D}$ contains triples $(u,i,j)$ where user $u$ prefers\n",
    "item $i$ over item $j$.\n",
    "\n",
    "**Mathematical Properties**: - **Probabilistic**: Based on maximum\n",
    "likelihood estimation - **Smooth**: Differentiable everywhere -\n",
    "**Pairwise**: Considers relative preferences\n",
    "\n",
    "#### List-wise Ranking Losses\n",
    "\n",
    "**ListNet Loss**:\n",
    "$$\\mathcal{L}_{\\text{ListNet}} = -\\sum_{u=1}^N \\sum_{i=1}^{n_u} P(i) \\log(\\hat{P}(i))$$\n",
    "\n",
    "where $P(i)$ and $\\hat{P}(i)$ are the true and predicted ranking\n",
    "distributions.\n",
    "\n",
    "**LambdaRank Loss**:\n",
    "$$\\mathcal{L}_{\\text{LambdaRank}} = \\sum_{(u,i,j) \\in \\mathcal{D}} \\lambda_{ij} \\log(\\sigma(\\hat{r}_{ui} - \\hat{r}_{uj}))$$\n",
    "\n",
    "where $\\lambda_{ij}$ is the lambda gradient that considers ranking\n",
    "metrics.\n",
    "\n",
    "### Multi-task Learning Losses\n",
    "\n",
    "#### Weighted Sum\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}} = \\sum_{k=1}^K \\lambda_k \\mathcal{L}_k$$\n",
    "\n",
    "where $\\lambda_k$ are task-specific weights.\n",
    "\n",
    "#### Uncertainty Weighting\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}} = \\sum_{k=1}^K \\frac{1}{2\\sigma_k^2} \\mathcal{L}_k + \\log(\\sigma_k)$$\n",
    "\n",
    "where $\\sigma_k$ is the uncertainty for task $k$.\n",
    "\n",
    "### Regularization Techniques\n",
    "\n",
    "#### L1 Regularization (Lasso)\n",
    "\n",
    "$$R_{\\text{L1}}(\\theta) = \\sum_{i=1}^p |\\theta_i|$$\n",
    "\n",
    "**Properties**: - **Sparsity**: Encourages sparse solutions - **Feature\n",
    "Selection**: Can zero out irrelevant features - **Non-differentiable**:\n",
    "Requires special optimization\n",
    "\n",
    "#### L2 Regularization (Ridge)\n",
    "\n",
    "$$R_{\\text{L2}}(\\theta) = \\sum_{i=1}^p \\theta_i^2$$\n",
    "\n",
    "**Properties**: - **Smooth**: Differentiable everywhere - **Shrinkage**:\n",
    "Reduces parameter magnitudes - **Stability**: Improves numerical\n",
    "stability\n",
    "\n",
    "#### Elastic Net\n",
    "\n",
    "$$R_{\\text{Elastic}}(\\theta) = \\alpha \\sum_{i=1}^p |\\theta_i| + (1-\\alpha) \\sum_{i=1}^p \\theta_i^2$$\n",
    "\n",
    "where $\\alpha \\in [0,1]$ controls the balance between L1 and L2.\n",
    "\n",
    "#### Dropout Regularization\n",
    "\n",
    "**Training**:\n",
    "$$\\mathbf{h}_{\\text{dropout}} = \\mathbf{h} \\odot \\mathbf{m}$$\n",
    "\n",
    "where $\\mathbf{m} \\sim \\text{Bernoulli}(p)$.\n",
    "\n",
    "**Inference**:\n",
    "$$\\mathbf{h}_{\\text{inference}} = p \\cdot \\mathbf{h}$$\n",
    "\n",
    "**Mathematical Properties**: - **Stochastic**: Introduces randomness\n",
    "during training - **Ensemble Effect**: Approximates ensemble of\n",
    "sub-networks - **Prevents Overfitting**: Reduces co-adaptation of\n",
    "neurons\n",
    "\n",
    "#### Batch Normalization\n",
    "\n",
    "**Training**:\n",
    "$$\\text{BN}(\\mathbf{x}) = \\gamma \\frac{\\mathbf{x} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "where $\\mu_B$ and $\\sigma_B^2$ are batch statistics.\n",
    "\n",
    "**Inference**:\n",
    "$$\\text{BN}(\\mathbf{x}) = \\gamma \\frac{\\mathbf{x} - \\mu_{\\text{pop}}}{\\sqrt{\\sigma_{\\text{pop}}^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "where $\\mu_{\\text{pop}}$ and $\\sigma_{\\text{pop}}^2$ are population\n",
    "statistics.\n",
    "\n",
    "### Optimization Algorithms\n",
    "\n",
    "#### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha_t \\nabla \\mathcal{L}(\\theta_t)$$\n",
    "\n",
    "**Properties**: - **Simple**: Easy to implement and understand -\n",
    "**Noisy**: Stochastic updates help escape local minima - **Memory\n",
    "Efficient**: Low memory requirements\n",
    "\n",
    "#### Adam Optimizer\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla \\mathcal{L}(\\theta_t)\n",
    "v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla \\mathcal{L}(\\theta_t))^2\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} m_t$$\n",
    "\n",
    "**Properties**: - **Adaptive**: Learning rate adapts to each parameter -\n",
    "**Momentum**: Incorporates momentum for faster convergence - **Robust**:\n",
    "Works well across different architectures\n",
    "\n",
    "#### RMSprop\n",
    "\n",
    "$$v_t = \\rho v_{t-1} + (1-\\rho) (\\nabla \\mathcal{L}(\\theta_t))^2\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} \\nabla \\mathcal{L}(\\theta_t)$$\n",
    "\n",
    "**Properties**: - **Adaptive**: Learning rate adapts to gradient\n",
    "magnitude - **Stable**: Good for non-convex optimization - **Memory\n",
    "Efficient**: Only stores gradient statistics\n",
    "\n",
    "### Learning Rate Scheduling\n",
    "\n",
    "#### Exponential Decay\n",
    "\n",
    "$$\\alpha_t = \\alpha_0 \\cdot \\gamma^t$$\n",
    "\n",
    "where $\\gamma \\in (0,1)$ is the decay rate.\n",
    "\n",
    "#### Cosine Annealing\n",
    "\n",
    "$$\\alpha_t = \\alpha_{\\min} + \\frac{1}{2}(\\alpha_{\\max} - \\alpha_{\\min})(1 + \\cos(\\frac{t}{T}\\pi))$$\n",
    "\n",
    "where $T$ is the total number of steps.\n",
    "\n",
    "#### Step Decay\n",
    "\n",
    "$$\\alpha_t = \\alpha_0 \\cdot \\gamma^{\\lfloor t/s \\rfloor}$$\n",
    "\n",
    "where $s$ is the step size.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "#### Rating Prediction Metrics\n",
    "\n",
    "**Mean Absolute Error (MAE)**:\n",
    "$$\\text{MAE} = \\frac{1}{N} \\sum_{(u,i) \\in \\mathcal{R}} |r_{ui} - \\hat{r}_{ui}|$$\n",
    "\n",
    "**Root Mean Square Error (RMSE)**:\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{(u,i) \\in \\mathcal{R}} (r_{ui} - \\hat{r}_{ui})^2}$$\n",
    "\n",
    "**Mean Absolute Percentage Error (MAPE)**:\n",
    "$$\\text{MAPE} = \\frac{100\\%}{N} \\sum_{(u,i) \\in \\mathcal{R}} \\left|\\frac{r_{ui} - \\hat{r}_{ui}}{r_{ui}}\\right|$$\n",
    "\n",
    "#### Ranking Metrics\n",
    "\n",
    "**Precision@k**:\n",
    "$$\\text{Precision@k} = \\frac{|\\text{relevant items in top-k}|}{k}$$\n",
    "\n",
    "**Recall@k**:\n",
    "$$\\text{Recall@k} = \\frac{|\\text{relevant items in top-k}|}{|\\text{total relevant items}|}$$\n",
    "\n",
    "**Normalized Discounted Cumulative Gain (NDCG@k)**:\n",
    "$$\\text{NDCG@k} = \\frac{\\text{DCG@k}}{\\text{IDCG@k}}$$\n",
    "\n",
    "where:\n",
    "$$\\text{DCG@k} = \\sum_{i=1}^k \\frac{2^{rel_i} - 1}{\\log_2(i+1)}$$\n",
    "\n",
    "**Mean Reciprocal Rank (MRR)**:\n",
    "$$\\text{MRR} = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{\\text{rank} _i}$$\n",
    "\n",
    "#### Diversity and Novelty Metrics\n",
    "\n",
    "**Intra-list Diversity**:\n",
    "$$\\text{Diversity@k} = \\frac{2}{k(k-1)} \\sum_{i=1}^k \\sum_{j=i+1}^k (1 - \\text{sim}(i,j))$$\n",
    "\n",
    "**Novelty**:\n",
    "$$\\text{Novelty@k} = -\\frac{1}{k} \\sum_{i=1}^k \\log_2(p(i))$$\n",
    "\n",
    "where $p(i)$ is the popularity of item $i$.\n",
    "\n",
    "### Hyperparameter Optimization\n",
    "\n",
    "#### Grid Search\n",
    "\n",
    "$$\\mathcal{H}^* = \\arg\\max_{\\mathcal{H} \\in \\mathcal{S}} \\text{Performance}(\\mathcal{H})$$\n",
    "\n",
    "where $\\mathcal{S}$ is the grid of hyperparameter combinations.\n",
    "\n",
    "#### Random Search\n",
    "\n",
    "$$\\mathcal{H}^* = \\arg\\max_{\\mathcal{H} \\sim p(\\mathcal{H})} \\text{Performance}(\\mathcal{H})$$\n",
    "\n",
    "where $p(\\mathcal{H})$ is the prior distribution over hyperparameters.\n",
    "\n",
    "#### Bayesian Optimization\n",
    "\n",
    "$$\\mathcal{H}^* = \\arg\\max_{\\mathcal{H}} \\text{Acquisition}(\\mathcal{H} | \\mathcal{D})$$\n",
    "\n",
    "where $\\text{Acquisition}$ is the acquisition function (e.g., Expected\n",
    "Improvement).\n",
    "\n",
    "### Theoretical Analysis\n",
    "\n",
    "#### 1. Convergence Properties\n",
    "\n",
    "**Theorem**: Under certain conditions, gradient descent converges to a\n",
    "stationary point.\n",
    "\n",
    "**Conditions**: - Loss function is Lipschitz continuous - Learning rate\n",
    "is sufficiently small - Gradients are bounded\n",
    "\n",
    "#### 2. Generalization Bounds\n",
    "\n",
    "**Theorem**: With probability at least $1-\\delta$:\n",
    "$$\\mathbb{E}[\\mathcal{L}(\\hat{f})] \\leq \\hat{\\mathcal{L}}(\\hat{f}) + O\\left(\\sqrt{\\frac{W \\log(W) + \\log(1/\\delta)}{n}}\\right)$$\n",
    "\n",
    "where $W$ is the number of parameters and $n$ is the number of training\n",
    "samples.\n",
    "\n",
    "#### 3. Optimization Landscape\n",
    "\n",
    "**Theorem**: For certain loss functions, the optimization landscape has\n",
    "good properties.\n",
    "\n",
    "**Properties**: - **Local Minima**: Most local minima are good -\n",
    "**Saddle Points**: Most critical points are saddle points - **Global\n",
    "Minima**: Global minima are well-behaved\n",
    "\n",
    "This comprehensive mathematical foundation provides the theoretical\n",
    "understanding and practical guidance needed to evaluate and optimize\n",
    "deep recommender systems effectively.\n",
    "\n",
    "### Hyperparameter Optimization\n",
    "\n",
    "#### Bayesian Optimization\n",
    "\n",
    "$$\\alpha^* = \\arg\\max_{\\alpha} \\text{Acquisition}(\\alpha | \\mathcal{D})$$\n",
    "\n",
    "#### Neural Architecture Search (NAS)\n",
    "\n",
    "$$\\mathcal{A}^* = \\arg\\max_{\\mathcal{A}} \\text{Performance}(\\mathcal{A})$$\n",
    "\n",
    "## 13.7.9. Production Considerations\n",
    "\n",
    "### Motivation and Challenges\n",
    "\n",
    "Deploying deep recommender systems in production environments presents\n",
    "unique challenges that differ from research settings:\n",
    "\n",
    "1.  **Scalability**: Must handle millions of users and items in\n",
    "    real-time\n",
    "2.  **Latency**: Response times must be under 100ms for good user\n",
    "    experience\n",
    "3.  **Reliability**: System must be robust and fault-tolerant\n",
    "4.  **Cost**: Computational resources must be optimized for cost\n",
    "    efficiency\n",
    "5.  **Freshness**: Models must stay current with changing user\n",
    "    preferences\n",
    "\n",
    "### Model Serving Architecture\n",
    "\n",
    "#### Mathematical Foundation\n",
    "\n",
    "**Inference Pipeline**:\n",
    "$$\\mathbf{y} = f_{\\text{model}}(\\mathbf{x}) = f_{\\text{post}}(f_{\\text{model}}(f_{\\text{pre}}(\\mathbf{x})))$$\n",
    "\n",
    "where: - $f_{\\text{pre}}$: Preprocessing function - $f_{\\text{model}}$:\n",
    "Model inference - $f_{\\text{post}}$: Post-processing function\n",
    "\n",
    "**Batch Processing**:\n",
    "$$\\mathbf{Y} = f_{\\text{model}}(\\mathbf{X}) \\in \\mathbb{R}^{B \\times d_{\\text{out}}}$$\n",
    "\n",
    "where $B$ is the batch size.\n",
    "\n",
    "#### Model Serving Strategies\n",
    "\n",
    "**1. TensorFlow Serving**\n",
    "\n",
    "**Model Export**:\n",
    "\n",
    "``` python\n",
    "# Save model\n",
    "model.save('recommendation_model')\n",
    "\n",
    "# Load and serve\n",
    "import tensorflow as tf\n",
    "loaded_model = tf.keras.models.load_model('recommendation_model')\n",
    "```\n",
    "\n",
    "**Mathematical Properties**: - **Versioning**: Supports model versioning\n",
    "for A/B testing - **Batching**: Efficient batch processing - **GPU\n",
    "Support**: Optimized for GPU inference\n",
    "\n",
    "**2. ONNX Export**\n",
    "\n",
    "**Conversion Process**:\n",
    "\n",
    "``` python\n",
    "import onnx\n",
    "import tf2onnx\n",
    "\n",
    "# Convert to ONNX\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model)\n",
    "onnx.save(onnx_model, \"model.onnx\")\n",
    "```\n",
    "\n",
    "**Mathematical Properties**: - **Interoperability**: Works across\n",
    "different frameworks - **Optimization**: ONNX Runtime provides\n",
    "optimizations - **Portability**: Can be deployed on various platforms\n",
    "\n",
    "**3. TorchServe**\n",
    "\n",
    "**Model Packaging**:\n",
    "\n",
    "``` python\n",
    "# Create model archive\n",
    "torch-model-archiver --model-name recommendation --version 1.0 --model-file model.pt --handler recommendation_handler.py\n",
    "```\n",
    "\n",
    "### Scalability Solutions\n",
    "\n",
    "#### Model Parallelism\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "$$\\mathbf{y} = f_2(f_1(\\mathbf{x}))$$\n",
    "\n",
    "where $f_1$ and $f_2$ run on different devices.\n",
    "\n",
    "**Pipeline Parallelism**:\n",
    "$$\\mathbf{y}_i = f_i(\\mathbf{y}_{i-1})$$\n",
    "\n",
    "where each $f_i$ runs on a different device.\n",
    "\n",
    "**Mathematical Properties**: - **Throughput**: Increases inference\n",
    "throughput - **Memory**: Reduces memory requirements per device -\n",
    "**Communication**: Requires inter-device communication\n",
    "\n",
    "#### Data Parallelism\n",
    "\n",
    "**Gradient Aggregation**:\n",
    "$$\\nabla \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N \\nabla \\mathcal{L}_i$$\n",
    "\n",
    "**Inference Parallelism**:\n",
    "$$\\mathbf{Y} = [f(\\mathbf{x}_1), f(\\mathbf{x}_2), \\ldots, f(\\mathbf{x}_N)]$$\n",
    "\n",
    "where each $f(\\mathbf{x}_i)$ runs on a different device.\n",
    "\n",
    "**Mathematical Properties**: - **Scalability**: Linear scaling with\n",
    "number of devices - **Independence**: No communication between devices -\n",
    "**Load Balancing**: Easy to distribute workload\n",
    "\n",
    "#### Distributed Training\n",
    "\n",
    "**AllReduce Algorithm**:\n",
    "$$\\mathbf{g}_{\\text{global}} = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{g}_i$$\n",
    "\n",
    "where $\\mathbf{g}_i$ is the gradient from device $i$.\n",
    "\n",
    "**Ring AllReduce**:\n",
    "$$\\mathbf{g}_{\\text{global}} = \\text{ReduceScatter}(\\text{AllGather}(\\mathbf{g}_{\\text{local}}))$$\n",
    "\n",
    "### Real-time Recommendations\n",
    "\n",
    "#### Online Learning\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta_t \\nabla \\mathcal{L}(\\theta_t, \\mathbf{x}_t, y_t)$$\n",
    "\n",
    "**Stochastic Gradient Descent**:\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\nabla \\mathcal{L}(\\theta_t, \\mathbf{x}_t, y_t)$$\n",
    "\n",
    "**Adaptive Learning Rate**:\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} \\nabla \\mathcal{L}(\\theta_t, \\mathbf{x}_t, y_t)$$\n",
    "\n",
    "where $v_t = \\beta v_{t-1} + (1-\\beta) \\nabla \\mathcal{L}(\\theta_t)^2$.\n",
    "\n",
    "#### Incremental Updates\n",
    "\n",
    "**Exponential Moving Average**:\n",
    "$$\\mathbf{h}_{t+1} = \\beta \\mathbf{h}_t + (1-\\beta) \\text{update}(\\mathbf{x}_{t+1})$$\n",
    "\n",
    "**Kalman Filter**:\n",
    "$$\\mathbf{h}_{t+1} = \\mathbf{h}_t + \\mathbf{K}_t (\\mathbf{z}_t - \\mathbf{H}_t \\mathbf{h}_t)$$\n",
    "\n",
    "where $\\mathbf{K}_t$ is the Kalman gain.\n",
    "\n",
    "### Caching and Optimization\n",
    "\n",
    "#### Embedding Caching\n",
    "\n",
    "**Cache Hit Rate**:\n",
    "$$\\text{Hit Rate} = \\frac{\\text{Cache Hits}}{\\text{Total Requests}}$$\n",
    "\n",
    "**Cache Size Optimization**:\n",
    "$$\\text{Memory Usage} = \\sum_{i=1}^N d_i \\cdot \\text{sizeof}(\\text{float})$$\n",
    "\n",
    "where $d_i$ is the embedding dimension for item $i$.\n",
    "\n",
    "#### Quantization\n",
    "\n",
    "**Post-training Quantization**:\n",
    "$$\\mathbf{W}_{\\text{quantized}} = \\text{round}\\left(\\frac{\\mathbf{W} - \\min(\\mathbf{W})}{\\max(\\mathbf{W}) - \\min(\\mathbf{W})} \\cdot (2^b - 1)\\right)$$\n",
    "\n",
    "where $b$ is the number of bits.\n",
    "\n",
    "**Dynamic Quantization**:\n",
    "$$\\mathbf{x}_{\\text{quantized}} = \\text{round}\\left(\\frac{\\mathbf{x}}{\\text{scale}} + \\text{zero\\_point}\\right)$$\n",
    "\n",
    "### Monitoring and Observability\n",
    "\n",
    "#### Performance Metrics\n",
    "\n",
    "**Latency**:\n",
    "$$\\text{Latency} = \\frac{1}{N} \\sum_{i=1}^N t_i$$\n",
    "\n",
    "where $t_i$ is the response time for request $i$.\n",
    "\n",
    "**Throughput**:\n",
    "$$\\text{Throughput} = \\frac{\\text{Number of Requests}}{\\text{Time Period}}$$\n",
    "\n",
    "**Error Rate**:\n",
    "$$\\text{Error Rate} = \\frac{\\text{Number of Errors}}{\\text{Total Requests}}$$\n",
    "\n",
    "#### Model Performance Monitoring\n",
    "\n",
    "**Prediction Drift**:\n",
    "$$\\text{Drift} = \\|\\mu_{\\text{training}} - \\mu_{\\text{production}}\\|_2$$\n",
    "\n",
    "where $\\mu$ represents the mean of predictions.\n",
    "\n",
    "**Data Drift**:\n",
    "$$\\text{Data Drift} = \\text{KL}(P_{\\text{training}} \\| P_{\\text{production}})$$\n",
    "\n",
    "where $\\text{KL}$ is the Kullback-Leibler divergence.\n",
    "\n",
    "### A/B Testing Framework\n",
    "\n",
    "#### Statistical Testing\n",
    "\n",
    "**T-test for Mean Comparison**:\n",
    "$$t = \\frac{\\bar{x}_A - \\bar{x}_B}{\\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}}}$$\n",
    "\n",
    "where $\\bar{x}_A, \\bar{x}_B$ are sample means and $s_A^2, s_B^2$ are\n",
    "sample variances.\n",
    "\n",
    "**Chi-square Test for Proportions**:\n",
    "$$\\chi^2 = \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}$$\n",
    "\n",
    "where $O_i$ and $E_i$ are observed and expected frequencies.\n",
    "\n",
    "#### Multi-armed Bandit Testing\n",
    "\n",
    "**Upper Confidence Bound (UCB)**:\n",
    "$$\\text{UCB}_i = \\bar{x}_i + \\sqrt{\\frac{2 \\log(t)}{n_i}}$$\n",
    "\n",
    "where $\\bar{x}_i$ is the sample mean of arm $i$ and $n_i$ is the number\n",
    "of pulls.\n",
    "\n",
    "**Thompson Sampling**:\n",
    "$$\\theta_i \\sim \\text{Beta}(\\alpha_i, \\beta_i)$$\n",
    "\n",
    "where $\\alpha_i, \\beta_i$ are the parameters of the Beta distribution.\n",
    "\n",
    "### Cost Optimization\n",
    "\n",
    "#### Computational Cost\n",
    "\n",
    "**FLOPs Calculation**:\n",
    "$$\\text{FLOPs} = \\sum_{l=1}^L (2 \\cdot d_{l-1} \\cdot d_l + d_l)$$\n",
    "\n",
    "where $d_l$ is the dimension of layer $l$.\n",
    "\n",
    "**Memory Usage**:\n",
    "$$\\text{Memory} = \\sum_{l=1}^L (4 \\cdot d_{l-1} \\cdot d_l + 4 \\cdot d_l) \\text{ bytes}$$\n",
    "\n",
    "#### Cost-Effective Training\n",
    "\n",
    "**Gradient Accumulation**:\n",
    "$$\\mathbf{g}_{\\text{accumulated}} = \\sum_{i=1}^k \\mathbf{g}_i$$\n",
    "\n",
    "where $k$ is the accumulation steps.\n",
    "\n",
    "**Mixed Precision Training**:\n",
    "$$\\mathbf{g}_{\\text{fp16}} = \\text{cast\\_to\\_fp16}(\\mathbf{g}_{\\text{fp32}})$$\n",
    "\n",
    "### Security and Privacy\n",
    "\n",
    "#### Differential Privacy\n",
    "\n",
    "**Laplace Mechanism**:\n",
    "$$f_{\\text{DP}}(\\mathbf{x}) = f(\\mathbf{x}) + \\text{Lap}\\left(\\frac{\\Delta f}{\\epsilon}\\right)$$\n",
    "\n",
    "where $\\Delta f$ is the sensitivity and $\\epsilon$ is the privacy\n",
    "parameter.\n",
    "\n",
    "**Gaussian Mechanism**:\n",
    "$$f_{\\text{DP}}(\\mathbf{x}) = f(\\mathbf{x}) + \\mathcal{N}\\left(0, \\frac{\\Delta f^2 \\log(1/\\delta)}{2\\epsilon^2}\\right)$$\n",
    "\n",
    "#### Federated Learning\n",
    "\n",
    "**Federated Averaging**:\n",
    "$$\\mathbf{w}_{\\text{global}} = \\sum_{i=1}^N \\frac{n_i}{n} \\mathbf{w}_i$$\n",
    "\n",
    "where $n_i$ is the number of samples for client $i$.\n",
    "\n",
    "### Deployment Strategies\n",
    "\n",
    "#### Blue-Green Deployment\n",
    "\n",
    "**Traffic Splitting**:\n",
    "$$\\text{Traffic}_A = \\alpha \\cdot \\text{Total Traffic}\n",
    "\\text{Traffic}_B = (1-\\alpha) \\cdot \\text{Total Traffic}$$\n",
    "\n",
    "where $\\alpha$ is the traffic split ratio.\n",
    "\n",
    "#### Canary Deployment\n",
    "\n",
    "**Gradual Rollout**:\n",
    "$$\\text{Canary Traffic} = \\text{Total Traffic} \\cdot \\text{rollout\\_percentage}$$\n",
    "\n",
    "#### Rolling Updates\n",
    "\n",
    "**Batch Update**:\n",
    "$$\\text{Update Batch} = \\frac{\\text{Total Instances}}{\\text{Number of Batches}}$$\n",
    "\n",
    "### Theoretical Guarantees\n",
    "\n",
    "#### 1. Latency Bounds\n",
    "\n",
    "**Theorem**: Under certain conditions, the inference latency is bounded\n",
    "by:\n",
    "$$\\text{Latency} \\leq O(\\text{model_complexity} + \\text{data_size})$$\n",
    "\n",
    "#### 2. Throughput Analysis\n",
    "\n",
    "**Theorem**: The maximum throughput is given by:\n",
    "$$\\text{Throughput} = \\frac{\\text{Number of Workers}}{\\text{Latency per Request}}$$\n",
    "\n",
    "#### 3. Cost Analysis\n",
    "\n",
    "**Theorem**: The total cost is bounded by:\n",
    "$$\\text{Cost} = O(\\text{compute\\_cost} + \\text{memory_cost} + \\text{network_cost})$$\n",
    "\n",
    "This comprehensive mathematical foundation provides the theoretical\n",
    "understanding and practical guidance needed to deploy deep recommender\n",
    "systems in production environments effectively.\n",
    "\n",
    "## 13.7.10. Summary and Future Directions\n",
    "\n",
    "### Mathematical Summary\n",
    "\n",
    "#### Core Mathematical Principles\n",
    "\n",
    "Deep recommender systems are built on several fundamental mathematical\n",
    "principles:\n",
    "\n",
    "1.  **Universal Approximation**: Neural networks can approximate any\n",
    "    continuous function\n",
    "    $$|f(\\mathbf{x}) - \\sum_{i=1}^N \\alpha_i \\sigma(\\mathbf{w}_i^T \\mathbf{x} + b_i)| < \\epsilon$$\n",
    "\n",
    "2.  **Representation Learning**: Hierarchical feature learning through\n",
    "    multiple layers\n",
    "    $$\\mathbf{h}^{(l+1)} = \\sigma(\\mathbf{W}^{(l)} \\mathbf{h}^{(l)} + \\mathbf{b}^{(l)})$$\n",
    "\n",
    "3.  **Attention Mechanisms**: Weighted aggregation of information\n",
    "    $$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n",
    "\n",
    "4.  **Graph Neural Networks**: Message passing on structured data\n",
    "    $$\\mathbf{h}_i^{(l+1)} = \\sigma\\left(\\mathbf{W}^{(l)} \\sum_{j \\in \\mathcal{N}(i)} \\frac{1}{\\sqrt{|\\mathcal{N}(i)||\\mathcal{N}(j)|}} \\mathbf{h}_j^{(l)}\\right)$$\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "#### 1. Non-linear Modeling\n",
    "\n",
    "**Mathematical Foundation**: Captures complex interaction patterns that\n",
    "linear models cannot\n",
    "$$\\hat{r}_{ui} = f_{\\text{non-linear}}(\\mathbf{u}_u, \\mathbf{v}_i) \\neq \\mathbf{u}_u^T \\mathbf{v}_i$$\n",
    "\n",
    "**Practical Impact**: Can model complex user preferences and item\n",
    "characteristics\n",
    "\n",
    "#### 2. Automatic Feature Learning\n",
    "\n",
    "**Mathematical Foundation**: Discovers optimal representations\n",
    "automatically\n",
    "$$\\mathbf{f}_{\\text{learned}} = \\text{Encoder}(\\mathbf{x}_{\\text{raw}}) \\in \\mathbb{R}^d$$\n",
    "\n",
    "**Practical Impact**: Reduces manual feature engineering effort\n",
    "\n",
    "#### 3. Multi-modal Integration\n",
    "\n",
    "**Mathematical Foundation**: Combines various data types in unified\n",
    "framework\n",
    "$$\\mathbf{f}_{\\text{fused}} = \\text{Fusion}(\\mathbf{f}_{\\text{text}}, \\mathbf{f}_{\\text{image}}, \\mathbf{f}_{\\text{audio}})$$\n",
    "\n",
    "**Practical Impact**: Leverages all available information for better\n",
    "recommendations\n",
    "\n",
    "#### 4. End-to-end Learning\n",
    "\n",
    "**Mathematical Foundation**: Optimizes entire pipeline jointly\n",
    "$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{recommendation}} + \\lambda \\mathcal{L}_{\\text{auxiliary}}$$\n",
    "\n",
    "**Practical Impact**: Better performance through joint optimization\n",
    "\n",
    "#### 5. Scalability\n",
    "\n",
    "**Mathematical Foundation**: Can handle large-scale data efficiently\n",
    "$$\\text{Complexity} = O(n \\log n) \\text{ vs } O(n^2) \\text{ for traditional methods}$$\n",
    "\n",
    "**Practical Impact**: Can process millions of users and items\n",
    "\n",
    "### Key Challenges\n",
    "\n",
    "#### 1. Computational Cost\n",
    "\n",
    "**Mathematical Challenge**: Training deep models is computationally\n",
    "expensive\n",
    "$$\\text{FLOPs} = \\sum_{l=1}^L (2 \\cdot d_{l-1} \\cdot d_l + d_l)$$\n",
    "\n",
    "**Solutions**: - Model compression and quantization - Efficient\n",
    "architectures (e.g., sparse attention) - Distributed training\n",
    "\n",
    "#### 2. Interpretability\n",
    "\n",
    "**Mathematical Challenge**: Black-box nature makes explanations\n",
    "difficult\n",
    "$$\\text{Interpretability} = f(\\text{Model Complexity}, \\text{Feature Importance})$$\n",
    "\n",
    "**Solutions**: - Attention mechanisms for feature importance - SHAP\n",
    "values for feature attribution - Model-agnostic interpretability methods\n",
    "\n",
    "#### 3. Data Requirements\n",
    "\n",
    "**Mathematical Challenge**: Needs large amounts of training data\n",
    "$$n \\geq O\\left(\\frac{W \\log(W)}{\\epsilon^2}\\right)$$\n",
    "\n",
    "where $W$ is the number of parameters and $\\epsilon$ is the desired\n",
    "accuracy.\n",
    "\n",
    "**Solutions**: - Transfer learning from pre-trained models - Data\n",
    "augmentation techniques - Few-shot learning approaches\n",
    "\n",
    "#### 4. Hyperparameter Tuning\n",
    "\n",
    "**Mathematical Challenge**: Many parameters to optimize\n",
    "$$|\\mathcal{H}| = \\prod_{i=1}^k |\\mathcal{H}_i|$$\n",
    "\n",
    "where $\\mathcal{H}_i$ is the set of values for hyperparameter $i$.\n",
    "\n",
    "**Solutions**: - Bayesian optimization - Neural architecture search\n",
    "(NAS) - Automated hyperparameter tuning\n",
    "\n",
    "#### 5. Overfitting\n",
    "\n",
    "**Mathematical Challenge**: Risk of memorizing training data\n",
    "$$\\mathbb{E}[\\mathcal{L}(\\hat{f})] \\leq \\hat{\\mathcal{L}}(\\hat{f}) + O\\left(\\sqrt{\\frac{W \\log(W)}{n}}\\right)$$\n",
    "\n",
    "**Solutions**: - Regularization techniques (dropout, weight decay) -\n",
    "Early stopping - Data augmentation\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "#### 1. Start Simple\n",
    "\n",
    "**Mathematical Principle**: Begin with basic architectures and gradually\n",
    "increase complexity\n",
    "$$\\text{Model Complexity} = f(\\text{Data Size}, \\text{Problem Complexity})$$\n",
    "\n",
    "**Implementation**: - Start with NCF or simple MLP - Gradually add\n",
    "attention, GNNs, or transformers - Monitor performance vs. complexity\n",
    "trade-off\n",
    "\n",
    "#### 2. Use Pre-trained Models\n",
    "\n",
    "**Mathematical Principle**: Leverage transfer learning for better\n",
    "initialization\n",
    "$$\\theta_{\\text{init}} = \\theta_{\\text{pre-trained}} + \\Delta\\theta$$\n",
    "\n",
    "**Implementation**: - Use pre-trained embeddings (Word2Vec, BERT) -\n",
    "Fine-tune on recommendation task - Freeze early layers if data is\n",
    "limited\n",
    "\n",
    "#### 3. Regularize Properly\n",
    "\n",
    "**Mathematical Principle**: Prevent overfitting through regularization\n",
    "$$\\mathcal{L}_{\\text{reg}} = \\mathcal{L} + \\lambda \\sum_{\\theta} \\|\\theta\\|_2^2$$\n",
    "\n",
    "**Implementation**: - Use dropout (p = 0.1-0.5) - Apply weight decay (λ\n",
    "= 0.01-0.1) - Use batch normalization\n",
    "\n",
    "#### 4. Monitor Training\n",
    "\n",
    "**Mathematical Principle**: Track loss and metrics carefully\n",
    "$$\\text{Convergence} = \\frac{|\\mathcal{L}_t - \\mathcal{L}_{t-1}|}{|\\mathcal{L}_{t-1}|} < \\epsilon$$\n",
    "\n",
    "**Implementation**: - Monitor training and validation loss - Track\n",
    "ranking metrics (NDCG, Recall@k) - Use learning rate scheduling\n",
    "\n",
    "#### 5. Validate Thoroughly\n",
    "\n",
    "**Mathematical Principle**: Use multiple evaluation metrics\n",
    "$$\\text{Performance} = \\text{aggregate}(\\text{Accuracy}, \\text{Diversity}, \\text{Novelty})$$\n",
    "\n",
    "**Implementation**: - Use cross-validation - Evaluate on multiple\n",
    "metrics - Test on different user segments\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "#### 1. Self-supervised Learning\n",
    "\n",
    "**Mathematical Foundation**: Learning without explicit labels\n",
    "$$\\mathcal{L}_{\\text{self-supervised}} = \\mathcal{L}_{\\text{pretext}} + \\mathcal{L}_{\\text{downstream}}$$\n",
    "\n",
    "**Applications**: - Contrastive learning for user-item representations -\n",
    "Masked language modeling for sequential recommendations - Graph\n",
    "contrastive learning for collaborative filtering\n",
    "\n",
    "#### 2. Meta-learning\n",
    "\n",
    "**Mathematical Foundation**: Learning to learn recommendation patterns\n",
    "$$\\theta^* = \\arg\\min_\\theta \\mathbb{E}_{\\mathcal{T}} [\\mathcal{L}_{\\mathcal{T}}(\\theta)]$$\n",
    "\n",
    "**Applications**: - Few-shot learning for cold-start users - Adaptive\n",
    "architectures for different domains - Personalized model architectures\n",
    "\n",
    "#### 3. Federated Learning\n",
    "\n",
    "**Mathematical Foundation**: Privacy-preserving distributed training\n",
    "$$\\mathbf{w}_{\\text{global}} = \\sum_{i=1}^N \\frac{n_i}{n} \\mathbf{w}_i$$\n",
    "\n",
    "**Applications**: - Privacy-preserving recommendations - Cross-device\n",
    "personalization - Collaborative learning without data sharing\n",
    "\n",
    "#### 4. AutoML\n",
    "\n",
    "**Mathematical Foundation**: Automated architecture and hyperparameter\n",
    "search\n",
    "$$\\mathcal{A}^* = \\arg\\max_{\\mathcal{A}} \\text{Performance}(\\mathcal{A})$$\n",
    "\n",
    "**Applications**: - Neural architecture search for recommendation\n",
    "models - Automated hyperparameter optimization - End-to-end pipeline\n",
    "optimization\n",
    "\n",
    "#### 5. Explainable AI\n",
    "\n",
    "**Mathematical Foundation**: Interpretable recommendation explanations\n",
    "$$\\text{Explanation} = f(\\text{Model}, \\text{Input}, \\text{Output})$$\n",
    "\n",
    "**Applications**: - Attention-based explanations - Counterfactual\n",
    "explanations - Feature importance analysis\n",
    "\n",
    "### Theoretical Guarantees\n",
    "\n",
    "#### 1. Approximation Power\n",
    "\n",
    "**Theorem**: Deep recommender systems can approximate any continuous\n",
    "recommendation function to arbitrary precision.\n",
    "\n",
    "#### 2. Generalization Bounds\n",
    "\n",
    "**Theorem**: With proper regularization, deep models generalize well\n",
    "even with limited data.\n",
    "\n",
    "#### 3. Convergence Properties\n",
    "\n",
    "**Theorem**: Under appropriate conditions, training converges to good\n",
    "solutions.\n",
    "\n",
    "### Practical Impact\n",
    "\n",
    "Deep recommender systems have revolutionized recommendation technology\n",
    "by:\n",
    "\n",
    "1.  **Performance**: Achieving state-of-the-art results on benchmark\n",
    "    datasets\n",
    "2.  **Flexibility**: Handling diverse data types and recommendation\n",
    "    scenarios\n",
    "3.  **Scalability**: Processing millions of users and items efficiently\n",
    "4.  **Innovation**: Enabling new recommendation capabilities\n",
    "    (multi-modal, sequential)\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Deep recommender systems represent the cutting edge of recommendation\n",
    "technology, offering unprecedented ability to model complex user-item\n",
    "interactions. While they require significant computational resources and\n",
    "expertise, they can provide substantial improvements in recommendation\n",
    "quality when properly implemented and tuned.\n",
    "\n",
    "The mathematical foundations presented in this chapter provide the\n",
    "theoretical understanding needed to: - Design effective architectures\n",
    "for different recommendation scenarios - Optimize models for performance\n",
    "and efficiency - Deploy systems in production environments - Advance the\n",
    "field through research and innovation\n",
    "\n",
    "As the field continues to evolve, these mathematical principles will\n",
    "guide the development of even more sophisticated and effective\n",
    "recommendation systems.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "**Next**: [Introduction](01_introduction.md) - Return to the beginning\n",
    "to review the fundamentals of recommender systems."
   ],
   "id": "73e16ffa-a4d3-4975-a32b-46c849762a6e"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
